{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagaditya39/ESRGAN-ImageSR-Colab/blob/main/gan2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ujiPn8qYD3m",
        "outputId": "2326fc12-c6b1-4220-923e-c4f8b0e6b9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HflivmcCJQhl"
      },
      "outputs": [],
      "source": [
        "#utils.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "def read_image(img_path):\n",
        "    base=os.path.basename(img_path)\n",
        "    ext = os.path.splitext(base)[1]\n",
        "    assert ext in ['.png', '.jpg', '.jpeg', '.JPEG']\n",
        "    image = tf.io.read_file(img_path)\n",
        "    if ext == '.png':\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "    else:\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "    return image\n",
        "\n",
        "# def create_lr_hr_pair(img_path, scale):\n",
        "#     image = read_image(img_path)\n",
        "#     lr_height, lr_width = image.shape[0] // scale, image.shape[1] // scale\n",
        "#     hr_height, hr_width = lr_height * scale, lr_width * scale\n",
        "#     hr_image = image[:hr_height, :hr_width, :]\n",
        "#     lr_shape = [lr_height, lr_width]\n",
        "#     lr_image = tf.image.resize(hr_image, lr_shape, method=tf.image.ResizeMethod.BICUBIC)\n",
        "\n",
        "#     return lr_image, hr_image\n",
        "\n",
        "def scale_image_0_1_range(image):\n",
        "    image = image / 255\n",
        "    red_max = tf.reduce_max(image, axis=None)\n",
        "    red_min = tf.reduce_min(image, axis=None)\n",
        "    if red_max > 1 or red_min < 0:\n",
        "        image = tf.clip_by_value(\n",
        "            image, 0, 1, name=None\n",
        "        )\n",
        "    return image\n",
        "\n",
        "\n",
        "# def unscale_image_0_255_range(image):\n",
        "#     image = image * 255\n",
        "#     red_max = tf.reduce_max(image, axis=None)\n",
        "#     red_min = tf.reduce_min(image, axis=None)\n",
        "#     if red_max > 255 or red_min < 0:\n",
        "#         image = tf.clip_by_value(\n",
        "#             image, 0, 255, name=None\n",
        "#         )\n",
        "#     return image\n",
        "\n",
        "def tensor2img(tensor):\n",
        "    return (np.squeeze(tensor.numpy()).clip(0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "def save_image_grid(lr, hr, ref=None, save_path=None):\n",
        "    lr_title = \"lr: {}\".format(lr.shape)\n",
        "    hr_title = \"hr: {}\".format(hr.shape)\n",
        "    images = [lr, hr]\n",
        "    titles = [lr_title, hr_title]\n",
        "    if ref is not None:\n",
        "        ref_title = \"ref: {}\".format(ref.shape)\n",
        "        images += [ref]\n",
        "        titles += [ref_title]\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "\n",
        "    for i, (img, title) in enumerate(zip(images, titles)):\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(title, fontsize = 20)\n",
        "        axes[i].axis('off')\n",
        "    # fig.savefig(save_path, bbox_inches = 'tight', pad_inches = 0.25)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_models_for_epoch(base_path, epoch_num, is_gan=False):\n",
        "    # \"\"\"Loads model(s) for a specific epoch.\"\"\"\n",
        "    model_path = f\"{base_path}_epoch_{epoch_num}.h5\"\n",
        "    if not Path(model_path).exists():\n",
        "        raise ValueError(f\"Model for epoch {epoch_num} not found at {model_path}\")\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    if is_gan:  # Load discriminator as well for GAN training\n",
        "        disc_path = f\"{base_path.replace('esrgan', 'disc_gan')}_epoch_{epoch_num}.h5\"\n",
        "        discriminator = tf.keras.models.load_model(disc_path)\n",
        "        return model, discriminator\n",
        "    return model\n",
        "\n",
        "def get_available_epochs(base_path):\n",
        "    # \"\"\"Returns a list of available epoch numbers from saved model filenames.\"\"\"\n",
        "    model_dir = Path(base_path).parent\n",
        "    pattern = re.compile(r\"_epoch_(\\d+)\\.h5\")\n",
        "    epochs = []\n",
        "    for filename in model_dir.glob(\"*.h5\"):\n",
        "        match = pattern.search(filename.name)\n",
        "        if match:\n",
        "            epochs.append(int(match.group(1)))\n",
        "    return sorted(epochs)\n",
        "\n",
        "\n",
        "def download_saved_folder(saved_folder_path, download_location=\"local\", drive_folder_id=None):\n",
        "    # \"\"\"Downloads the saved folder to the specified location.\"\"\"\n",
        "\n",
        "    if download_location == \"local\":\n",
        "        # Download to local machine (assuming you are using Colab)\n",
        "        shutil.make_archive(saved_folder_path, 'zip', saved_folder_path)\n",
        "        zipped_filename = saved_folder_path + \".zip\"\n",
        "        files.download(zipped_filename)\n",
        "\n",
        "    elif download_location == \"drive\" and drive_folder_id:\n",
        "        # Download to Google Drive (Colab only)\n",
        "        drive.mount('/content/gdrive')  # Mount Google Drive\n",
        "        destination_path = \"/content/gdrive/My Drive/\" + drive_folder_id\n",
        "        shutil.make_archive(saved_folder_path, 'zip', saved_folder_path)\n",
        "        zipped_filename = saved_folder_path + \".zip\"\n",
        "        shutil.move(zipped_filename, destination_path)\n",
        "        print(f\"Saved folder uploaded to Google Drive folder ID: {drive_folder_id}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"[!] Invalid download location or missing Drive folder ID.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quRvGoGgJJQI"
      },
      "outputs": [],
      "source": [
        "#metrics.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def calculate_psnr(img1, img2):\n",
        "    #img1 and img2 have range [0, 255]\n",
        "    #psnr = 20 * np.log10(255.0 / np.sqrt(np.mean((img1 - img2)**2)))\n",
        "    return tf.image.psnr(img1, img2, max_val=255)\n",
        "\n",
        "def calculate_ssim(hr, generated_hr):\n",
        "    #hr and generated_hr have range [0, 255]\n",
        "    return tf.image.ssim(hr, generated_hr, max_val=255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "WsyXyCB6I-v_",
        "outputId": "6e4b3af5-bf22-4039-9f03-1fbb26285641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-9f07d8c92e3a>:61: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(labels)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAHJCAYAAACloWxtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfDklEQVR4nO3deVxU5f4H8M/MMKwDiAqIuKJsCsIA4oKKGi6poZKaltv1umVikrdsM9FstUzTfppLmtG1sriIa9nVa+a+i4qAqIAookAIM8gyM78/iEkSkIEZzgx83q8XL52zPOc78zDw4ZxnniPSaDQaEBEREZFOxEIXQERERGSKGKKIiIiI6oAhioiIiKgOGKKIiIiI6oAhioiIiKgOGKKIiIiI6oAhioiIiKgOGKKIiIiI6oAhioiIiKgOzIQugIjIEFxdXXXep1evXvjxxx8xZswYHDt2DNu3b0fv3r11aqM++xKRaWGIIqJGaezYsY8tu3fvHv73v/9Vu75z5841tvnpp59ixYoVeOWVV7BgwQK91ElEposhiogapZUrVz627OjRo9oQVdX6CqtWrUJRUVGdzmYRUdPBEEVE9DcMT0RUGxxYTkT0N2PGjIGrqyuOHj2qXebq6ooVK1YAAFasWAFXV1ft1/z582vd9uHDhzF9+nTI5XJ06NAB3bp1wz//+U+cPn1a30+DiAyMZ6KIiGph7NixuHz5Mq5cuYIuXbqga9eu2nXBwcG1amPp0qX48ssvIRaL4efnh+DgYGRmZuLnn3/G/v37sXz5cjz33HOGegpEpGcMUUREtbBy5Up8+umnuHLlCoYOHarzwPJvv/0WX375JTp06IANGzagS5cu2nXHjx/HlClT8Prrr6N79+5wc3PTd/lEZAC8nEdEZGBqtVp7KXDt2rWVAhQA9OzZE/Pnz0dJSQliYmKEKJGI6oAhiojIwC5duoSsrCztGKiq9OrVCwA4NorIhPByHhGRgaWlpQEAbt68+cRP/uXk5DRESUSkBwxRREQGplarAQBOTk4IDQ2tcdvmzZs3RElEpAcMUUREBta6dWsAgIODQ42TfBKRaeGYKCKiWpJKpQCAsrIynfbz9/dH8+bNkZycjKSkJEOURkQCYIgiIqolFxcXAEBycrJO+0mlUrzyyivQaDT45z//iZMnTz62jUqlwu+//44zZ87opVYiMjxeziMiqqX+/fvD2toa+/btw6hRo9CxY0dIJBJ07979iZNk/uMf/0BmZibWrl2L0aNHw9PTEx06dIClpSWys7Nx5coV5Ofn44MPPkBgYGADPSMiqg+GKCKiWnJ0dERMTAw+++wzJCQk4MyZM1Cr1SgrK6vVTONvv/02hgwZgq1bt+LkyZP43//+B6lUCicnJ/Tq1QthYWF4+umnG+CZEJE+iDQajUboIoiIiIhMDcdEEREREdUBQxQRERFRHTBEEREREdUBQxQRERFRHTBEEREREdUBQxQRERFRHXCeKAMpKytDfn4+LCwsIBYzqxIREZkCtVqN4uJi2Nvbw8ys5pjEEGUg+fn5uHnzptBlEBERUR106NABLVq0qHEbhigDsbCwAFDeCVZWVnptW6VSITk5GR4eHpBIJHptm3TH/jAu7A/jwv4wPuyTmhUVFeHmzZva3+M1YYgykIpLeFZWVrC2ttZr2yqVCgBgbW3NN4ARYH8YF/aHcWF/GB/2Se3UZigOB+sQERER1QFDFBEREVEd8HIeERHpjVqtxqP3ta+4dFTxLwmPfQKIRCK9fHKeIYqIiOotLy8P9+7de+wXs0ajgZmZGa5duwaRSCRQdfQo9kk5S0tLtG/fvl5hiiGKiIjqJS8vD9nZ2XB1dYWlpWWlX8wajQZFRUWwsrJq0r+wjQn7pPw1yMzMRHZ2Nlq1alXndhiiiIioXu7duwdXV1fIZLLH1mk0GojFYkgkkib7C9vYsE/KOTs74+bNm3B2dq7z68CB5UREVGdqtRoqlQqWlpZCl0KkE6lUCo1GU2kMn64YooiIqM4qfgE15TMaZNoYooiIiGowcOBA/Pbbb4Ice/jw4Th69KggxzYWnp6eSE1NFboMveOYKCIiIgPavXu30CVorV69GtevX8eyZcuELqVREOxMVElJCd566y0MHDgQcrkcw4cPx86dO6vd/uTJkxgxYgT8/Pzw7LPP4urVq/U6fkxMDPr27Qu5XI7IyEjk5+dXWePQoUMREhJSr2MREVHjVFZWJnQJWsZUS1MhWIgqKyuDk5MTvv76a5w9exZLlixBdHQ0zp0799i2eXl5mDNnDqZPn45Tp05hxIgRePHFF1FSUlKnYx85cgSrV6/GunXrcPjwYYjFYixevPix7davX//EOzgTEZFp0Wg02Lx5M4YMGYLg4GDMmDEDWVlZ2vUffPAB+vfvD7lcjtGjR+PkyZPadatXr8bcuXPxxhtvICgoCJs2bcLrr7+O6OhozJ07F3K5HM888wwSExO1+zx6KXH16tWIjIzE22+/jcDAQAwaNKjSpb7bt29j8uTJkMvlmDBhAlasWIFJkyZV+Txu3boFT09PxMbGYuDAgQgPD6+x/oMHD+LLL7/EL7/8gpCQEAwcOBBA+QmDFStWYODAgejRowcWLFhQ5YkFoPz38Ysvvoju3buje/fuGDt2LHJzcwEABQUFeOedd9CvXz8EBgbi+eefx8OHD7X7nj59GkOHDkVgYCD+9a9/VfodfvjwYURERCAoKAijR4/G6dOntesmTZqEzz77DBMnToS/vz8mTZqEnJwcfPjhhwgODsZTTz2FEydOaLcvLCzU1hESEoKlS5eiuLi4yudTX4KFKGtra7z88sto27YtRCIRgoKCEBAQUGWI2r9/P9q1a4dRo0bB3NwcU6dOhVqt1n7jPekN8XexsbGIiIhA165dIZPJEBUVhf3796OgoEC7zY0bN7Bnzx7MnDlT/0+eiKgR02g0eFhc9tdXiaryYz191XVAcExMDHbu3IlNmzbhyJEj6NKlC6KiorTru3btitjYWJw6dQojR47Eyy+/jKKiIu36gwcPok+fPjh58iSmTJkCANi1axemTp2K06dPo2fPnjVeLjt48CBCQ0Nx8uRJvPDCC3jzzTe161555RV07twZx48fxzvvvIPY2NgnPp/Dhw8jPj5eu2119Q8YMACzZs3C4MGDceTIERw4cAAAsGLFCly+fBnbt2/HoUOHIJVKsXTp0iqP9dVXX0Gj0eC3337D8ePHsXjxYlhYWAAAFi5ciJycHMTFxeHkyZNYsGBBpYksf/75Z2zbtg2//PILzp8/j/j4eADA1atX8a9//QtvvvkmTp48iXnz5uGll17ShjMA2LlzJ5YsWYJjx45BpVJh3Lhx6Ny5M44dO4bnn38e77zzjnbbN954AyUlJdizZw/27t2LtLQ0/N///d8TX8e6MJoxUUqlEpcuXcLkyZMfW5ecnAxvb2/tY5FIBE9PTyQnJ6N///6V3hDOzs5Ys2YNoqKisG3btiqPlZKSgn79+mkfd+jQAVKpFNevX4efnx8AIDo6Gq+99lq9P7arUqn0OrX+A0UJFq8/hqwcBST/uQt+IEZ4Gg3QwckMPj48lW4MeEuLhqVSqbQfE6/4WvjF77h6M8/gx/bu0BwfvhRSq08GVgQujUaDbdu2YeHChXB1dQUA7RmkzMxMtG7dGs8884x2vylTpuCLL77AtWvX4OPjA41Gg65du2LYsGEAAAsLC2g0Gjz11FMIDAwEAIwcORI//vhjpWNW/KvRaODv74+wsDDtth988AFyc3NRVFSE8+fPY8OGDTA3N4eXlxdGjBiBS5cuVRkYK5bNnTsXNjY22mVPqv/v9Xz33Xf48ccf0bx5cwDAvHnzMHjwYHz00UeQSCSVjmlmZoa8vDykpaXB09MTXbt2BQBkZ2fjv//9L44ePQoHBwcAQEBAQKU6Z86ciWbNmgEAQkNDcfnyZTz77LP47rvvMGbMGO3r179/f3h5eeHQoUMYNWoUNBoNRo0aBTc3NwBAWFgYfvjhBzz77LMAygfuf/zxxygsLMTDhw9x4MABHD9+XPuazJ49G6+//jrmz5//2Oun0Wge+1mhy88OowhRarUar7/+Onx9fdGnT5/H1iuVStjb21daZmtrC4VCAQDaN0SbNm0AAJGRkfD398ft27fRunXrKtuzs7Ortr24uDjIZDL079+/0inCukhOTq7X/n+XW1iG9LsFUKkBQK3XtqnuLqWV4tDRc2hhJxW6FPpTQkKC0CU0GWZmZigqKoJYLC7/xaRumJ9NarUKSqWy1iHq4cOHUCqVyMzMRFRUVKX9xGIx0tLS0KxZM2zduhVxcXG4f/8+AEChUCArKwtubm4oLS2Fk5MTlEqldl+VSoUWLVpUWqZUKrWPHz12aWkpHBwcKq0DgJycHNy/fx8ymQxisVi7vkWLFlCr1ZXarlBxqaxZs2aV1j+p/oqQUFRUpA1vzz33XKW2RSIRMjIy4OTkVGn5888/j8LCQsydOxdFRUUYNmwYXnrpJdy4cQMymQyWlpZV1goAMplMu87MzAw5OTlQKpVIT0/HmTNn8N1332m3LSsrQ/fu3aFUKqFWq2Fvb6/dVyKRoHnz5o8dJycnB1lZWVCpVOjfv3+ldSqV6rHt1Wo1SktL6/WzQvAQpdFosHjxYmRnZ2PTpk1Vvhmsra0rXWoDyq95VqTMijfEo6cNxWIxsrKyEB8fjy+//BIAEBgYiI0bN9bYXn5+Pj7//HPExMTo5fl5eHjA2tpaL21VCOhWhFPnLsHDw0MvN1Ck+vnix4u4mpYHpag5nvLvJHQ5TZ5KpUJCQgJ8fX0f+yua9E+lUuHatWuwsrLSvt4fR/ZDcUn5L2oNNCgqeggrK0uIoN9T5xbmtZ9xWyQSwdLSEtbW1nBxcUF0dDR69Ojx2HanT5/G5s2bsXXrVri7u0MsFiM4OBgWFhawtraGVCqFVCqt9HNdIpFUWmZlZQUA2sePHlsqlcLMzEy7ruI1s7S0RLt27VBYWAi1Wq2d/T0nJwdisbjK3yMVV0psbGy0l9SeVL+5ubn2mFZWVmjdujUsLS0RFxenPRFRE2tra7z55pt48803kZ6ejhkzZsDDwwOhoaEoLCxEcXGx9kzU31lZWWmfh1QqhUQigbW1Ndq0aQN/f39ERkZWuZ9YLIa5ubl2X3Nz80qvScWAeisrK3Ts2BFmZmY4evQozM3Na3wuKpUKUqkU3t7elX5WKJXKWp8AETREaTQaLFmyBImJidiyZYs2FP2dh4cHvv/++0r7JSUlYcKECQAAFxcXLFmypMo3REBAAGbPnl1pmbu7OxITE7WD8NLS0lBSUgI3NzdcuXIF2dnZGDt2LACgtLQUBQUFCAkJwZYtW+Du7q7Tc5RIJHr/Qd7MzgqO9lK0d7HnLwkjENzFGVfT8nAhJQej+nsIXQ79yRDvPaqaSCTSflU8trIs/wNPo9FAoyqFlYVU0Ak5H61twoQJWLlyJT7++GO0a9cO+fn5OHLkCIYNGwalUgkzMzM4ODhApVJh/fr1KCws1O77aDtVtV3dMavb/9F/XV1d4e/vj1WrVuG1117D9evXsWvXLnTs2LHK166qdp9Uf8uWLXH48GGo1WqIRCKIxWI899xz+PDDD7F48WI4OTkhJycH586d015yfNTBgwfRoUMHtG/fHra2ttow5OTkhAEDBiA6OhrR0dGwt7fHhQsX4OPjUynM/P11E4lEeO655zBr1iz07t0bAQEBKCkpwfnz59GhQwe0atWq2te8qtfQ0dERoaGheP/99/HKK6/Azs4OWVlZjw3jebSNv/+s0OXnhqCnMZYuXYoLFy5g06ZNVd5zqcKgQYOQlpaGHTt2oKSkBF9//TUAoHfv3gCACRMm4LPPPkN6ejoAID8/H3v27Km2vYiICMTGxuLKlStQKBRYuXIlBg0aBFtbW8jlchw8eBBxcXGIi4vDsmXL0KxZM8TFxaFjx456fPbUWMg9y093J6TmoKSU43CIjN2kSZMwfPhwzJo1CwEBARg5ciR+//13AECfPn0QGhqKp59+GgMHDoSZmRlcXFwarLZPPvkESUlJ6NGjB6Kjo/HMM8888YzKo55U/9ChQ2FmZoYBAwZg0KBBAIB//etf8PLywgsvvAC5XI7x48dXe4krLS0N06dPR0BAAMLDwxESEoKRI0cCAD766CPIZDKEh4ejR48eWLFiBdS1uLTbtWtXfPTRR1i+fDl69OiBAQMGYPPmzbXatyofffQRpFIpRo0ahcDAQPzzn//EzZs369TWk4g09ZnvvB4yMzMxcOBAmJubw8zsrxNis2bNwuzZsyGXy7FhwwYEBQUBAE6cOIF3330X6enpcHd3x7Jly7SDzdVqNb799lv8+9//xt27d2FnZ4fevXvj/fffr/b4MTExWLduHRQKBUJCQvDee+89Nu6q4rivvPIKjhw5otPzUyqVSExMhLe3t94v56lUKpw/fx7+/v78S9sIlJWVYdLivSh8qMa7s3rB38PpyTuRwfD90bBUKhWSk5Ph4eFR5eut0WigVCphbW3NW8PUwbJly1BUVIT33ntPb22yT8pV972ry+9vwS7nubq6Iikpqdr1f5/qoEePHti1a1eV24rFYkyaNKnauTSqMnHiREycOPGJ2/Xo0UPnAEVNi0gkQicXS1y4ocTZpHsMUURUZ5cuXYJMJkP79u1x5swZxMXF4dNPPxW6LKqG4APLiRqDzhUh6updTHumq9DlEJGJysnJQWRkJHJzc9GyZUu89NJLCA0NFbosqgZDFJEeuLWygEgEpGUVICe/CC3srYQuiYhMUGhoKA4ePCh0GVRL/Hw8kR7YWErQuU0zAMDZq9nCFkNERA2CIYpIT+QejgCAs0kMUURETQFDFJGeBHiWh6jzyfegUnE2eWoaKj7dJdAHvYnqrOJ7tj6fUOSYKCI9cW/bDDaWZigsKkVKxh/w6tBc6JKIDE4sFsPS0hKZmZlwdnaGVFr51kcajQZqtRoqlapJf5zemLBPyl+DnJwcSKXSet35gyGKSE8kEjH8PZxw5OJtnE3KZoiiJqN9+/bIzs7GzZs3HzsjpdFoUFpaCqlU2BnL6S/sk3JSqRTt2rWrVxsMUUR6FOD1Z4i6mo3nh3gJXQ5RgxCLxWjVqhWcnZ3Lb/PySJCquJfh3+9PRsJhn0B7y5v6Yogi0qOAP28Bk5KRhweKEtjZ1P52DUSm7tH7mf0d72VofNgn9ceB5UR61LKZFdq1soVaA1xIvid0OUREZEAMUUR6VnE2ilMdEBE1bgxRRHr2V4i6y499ExE1YgxRRHrW1a0FzKUS5D4oxs07D4Quh4iIDIQhikjPzKUS+HZqAQA4x0t6RESNFkMUkQEEeJVf0jvD++gRETVaDFFEBlAxLurKjVwUFZcJXA0RERkCQxSRAbg6yuDU3BplKjUSUu8LXQ4RERkAQxSRAYhEIu3ZqHO8pEdE1CgxRBEZSEWIOsPB5UREjRJDFJGB+Lm3hEQswp37Cty5rxC6HCIi0jOGKCIDsbaUwqtDcwCcvZyIqDFiiCIyoMA/pzo4y3FRRESNDkMUkQHJ/xwXdfHaPZSWqQWuhoiI9IkhisiA3Frbo5nMAg9LVEi8mSN0OUREpEcMUUQGJBaL4O/pCICX9IiIGhuGKCIDC/zzkh4HlxMRNS4MUUQG5u9RHqJu3H6A3AcPBa6GiIj0hSGKyMCa2Vqgcxt7AMA5no0iImo0GKKIGkCAlzMAjosiImpMGKKIGoD2PnrJ96BSawSuhoiI9IEhiqgBeLZ3gLWlGQqUJUi99YfQ5RARkR4wRBE1ADOJGH7uf051wHFRRESNAkMUUQOpuKTHcVFERI0DQxRRA6kIUUlpuShUlghcDRER1RdDFFEDcWpujTZOMqg1wIWU+0KXQ0RE9SRYiIqJiUFERAR8fHwQFRVV7Xbx8fGQy+XaL39/f3h6euKXX36p17H79u0LuVyOyMhI5OfnP7ZNSUkJhg4dipCQkDofh+jvArzKz0aduXpX4EqIiKi+BAtRTk5OmDNnDsaNG1fjduHh4Th37pz2a9WqVZDJZOjbt2+djnvkyBGsXr0a69atw+HDhyEWi7F48eLHtlu/fj1atGhRp2MQVUc71UFSNjQaTnVARGTKBAtRgwcPRlhYGBwcHHTa76effsKwYcNgZWUFANBoNNi8eTOGDBmC4OBgzJgxA1lZWdXuHxsbi4iICHTt2hUymQxRUVHYv38/CgoKtNvcuHEDe/bswcyZM+v25Iiq4dOpJczNxLif/xDpdwuevAMRERktM6EL0EVeXh4OHDiAb775RrssJiYGO3fuxKZNm+Ds7Iw1a9YgKioK27Ztq7KNlJQU9OvXT/u4Q4cOkEqluH79Ovz8/AAA0dHReO2112BpaVnvmlUqFVQqVb3b+Xubj/5LwtKlP8zEQBe3FjiffA9nEu+ijaONoctrcvj+MC7sD+PDPqmZLq+LSYWonTt3ol27dpDL5dpl27Ztw8KFC9GmTRsAQGRkJPz9/XH79m20bt36sTaUSiXs7OwqLbO1tYVCoQAAxMXFQSaToX///jhx4kS9a05OTq53G9VJSEgwWNuku9r2h7Os/JN5h06nor3dA0OW1KTx/WFc2B/Gh31SfyYVoiouxT0qMzMTUVFREIv/ujIpFouRlZWF+Ph4fPnllwCAwMBAbNy4EdbW1pUu3QFAYWEhbGxskJ+fj88//xwxMTF6q9nDwwPW1tZ6aw8oT8kJCQnw9fWFRCLRa9ukO137o4VLAX4+ewgZ90vh3cUXFubsQ33i+8O4sD+MD/ukZkqlstYnQEwmRF25cgUpKSkYOXJkpeUuLi5YsmQJevTo8dg+AQEBmD17dqVl7u7uSExMRHh4OAAgLS0NJSUlcHNzw5UrV5CdnY2xY8cCAEpLS1FQUICQkBBs2bIF7u7uOtctkUgM9k1qyLZJd7Xtj/Yu9mjZzAr3/yjClZt5CPJ2boDqmh6+P4wL+8P4sE+qpstrItjA8rKyMhQXF6OsrAxqtRrFxcUoLS2tdvvY2Fj07dsXjo6OlZZPmDABn332GdLT0wEA+fn52LNnT7XtREREIDY2FleuXIFCocDKlSsxaNAg2NraQi6X4+DBg4iLi0NcXByWLVuGZs2aIS4uDh07dtTPE6cmTyQSIfDPqQ54CxgiItMlWIhau3YtunXrhnXr1mHfvn3o1q0bFi1aBACQy+U4ffq0dtuSkhLs3LkTY8aMeaydSZMmYfjw4Zg1axYCAgIwcuRI/P7779UeNyQkBJGRkZg5cyb69OmD0tJSLFmyBABgbm4OR0dH7Ze9vT3EYjEcHR1hZmYyJ+3IBMh5CxgiIpMnWDKIjIxEZGRklevOnTtX6bG5uXm1g7zFYjEmTZqESZMm1frYEydOxMSJE5+4XY8ePXDkyJFat0tUW37ujhCLRci8V4i7uUo4N9fvuDkiIjI83vaFSAAyKyk825XPkcZLekREpokhikgg2nFRvAUMEZFJYogiEkjFffQupNxHmUotcDVERKQrhigigXRybQY7G3MUFZfh6s1cocshIiIdMUQRCUQsFkHuwakOiIhMFUMUkYACvMrnPWOIIiIyPQxRRAKqOBOVeisfeQUPBa6GiIh0wRBFJCAHO0u4tbYHAJxPvidwNUREpAuGKCKBBXhx9nIiIlPEEEUksIBH7qOnVmsEroaIiGqLIYpIYF7tm8PKQoIHihJcz8wXuhwiIqolhigigUnNxOjWufxTemeSOHs5EZGpYIgiMgIVl/TOJXFwORGRqWCIIjICAZ7lISrxZi4URaUCV0NERLXBEEVkBFq1sIGrow3Uag0uXuPZKCIiU8AQRWQk5H+ejTrDqQ6IiEwCQxSRkQj0cgZQPtWBRsOpDoiIjB1DFJGR8HFrATOJGPfyinAru1DocoiI6AkYooiMhKWFGXzcWgDgDYmJiEwBQxSREakYF8UQRURk/BiiiIxI4J/zRV26dh/FpSqBqyEiopowRBEZkXatbNHC3hIlZWpcvp4jdDlERFQDhigiIyISibQTb57lVAdEREaNIYrIyFTcAuYs76NHRGTUGKKIjIy/uyPEIiDjbiGy85RCl0NERNVgiCIyMjJrc3i0cwAAnOOn9IiIjBZDFJERCuBUB0RERo8hisgIVYyLupB8DyqVWuBqiIioKgxRREaoc1sH2FpLoXhYhqT0PKHLISKiKjBEERkhiVgEfw9OdUBEZMwYooiMVMW4qDMcF0VEZJQYooiMlNzTEQCQeusP5BcWC1wNERH9HUMUkZFqYW+FDi520GiAc8n3hC6HiIj+hiGKyIhVXNLjfFFERMaHIYrIiP11C5hsqNUagashIqJHCRaiYmJiEBERAR8fH0RFRdW4raenJ/z9/SGXyyGXyzF9+vR6HXvfvn0ICwuDn58fpkyZgszMzCq3mzRpEjw9PVFczPEoJIwuHZvD0lyCPwqKcfPOA6HLISKiRwgWopycnDBnzhyMGzeuVtv/9NNPOHfuHM6dO4eNGzfW+bipqal44403EB0djRMnTsDT0xPz589/bLv//Oc/UKs5ySEJS2omgW/nlgCAM1d5Q2IiImMiWIgaPHgwwsLC4ODgUO+2duzYgREjRiAoKAjPP/88UlJSqt02Pj4effv2RZ8+fWBpaYl58+bh6tWrlfbJy8vD2rVr8dprr9W7NqL6CuQtYIiIjJKZ0AXU1pQpU6BWq+Hj44NXX30V7u7uAIADBw5g1apVWLt2LTp37ozt27dj9uzZ2Lt3L8zNzR9rJzk5Gb6+vtrHMpkM7dq1Q0pKirbNjz/+GFOmTEGLFi3qXbdKpYJKpap3O39v89F/SViG7g8/9/IzUYk3clGoKIaVpcm8bQXB94dxYX8YH/ZJzXR5XUzip/E333wDf39/lJSUYMOGDZg2bRr27t0LmUyGbdu2Yfr06fD09AQAjB8/Hhs3bsSFCxfQvXv3x9pSKpWws7OrtMzW1hYKhQIAcOrUKSQnJ+O9997D7du36117cnJyvduoTkJCgsHaJt0Zsj8cZBLkFaqw49dT8GpjZbDjNCZ8fxgX9ofxYZ/Un0mEqODgYACAubk5oqKiEB8fj7Nnz6Jfv37IzMzE8uXLsWLFCu32paWluHv3LuLj47F48WIAQOvWrbF7925YW1ujoKCgUvuFhYWwsbFBaWkplixZgvfffx9isX6udHp4eMDa2lovbVVQqVRISEiAr68vJBKJXtsm3TVEf/S6KcGeo2n4o1QGf3/fJ+/QhPH9YVzYH8aHfVIzpVJZ6xMgJhGi/k4kEkGjKf+4t4uLC6ZPn46IiIgqtw0PD6/02MPDA4mJidrHCoUC6enpcHd3x927d3H9+nW8+OKLAP46pTdw4EB88MEH6Nevn861SiQSg32TGrJt0p0h+yPQqxX2HE3DuaR7EIvFEIlEBjlOY8L3h3Fhfxgf9knVdHlNBAtRZWVlUKlUKCsrg1qtRnFxMcRiMaRSaaXtUlJSUFJSAk9PT5SWlmLjxo0oLi6GXC4HAEyYMAGffPIJunTpAk9PTyiVSpw4cQLBwcGQyWSPHTc8PBxjxozB0aNHERgYiNWrV8PT0xPu7u5QqVQ4dOiQdts7d+5g7Nix+OGHH+Do6GjYF4SoBr6dW8JMIsLdXCXu3FegtePj39tERNSwBAtRa9euxZo1a7SP9+3bh9GjR+PDDz+EXC7Hhg0bEBQUhJycHERHRyMrKwsWFhbw8fHBpk2btOOawsLC8PDhQyxcuBC3bt2ClZUVAgMDtZcA/65Tp054//33sWjRIty/fx9+fn5YuXIlgPL0+WhYqpgfqmXLllUOUidqKFYWZujSsQUuXruPM1ezGaKIiIyAYCEqMjISkZGRVa47d+6c9v89e/bEvn37amxrxIgRGDFiRK2P/fTTT+Ppp59+4nZt2rRBUlJSrdslMqQATydcvHYfZ5Oy8UxfN6HLISJq8njbFyITUXELmITU+ygp5UeTiYiExhBFZCI6uNjBwdYCxSUqXLmRI3Q5RERNHkMUkYkQiUSP3JD4nsDVEBERQxSRCQmouAUM76NHRCQ4higiE+Lv4QSRCEjLKkBOfpHQ5RARNWkMUUQmxM7GHO5tmwEAzl7lDYmJiITEEEVkYgI8nQEAZ5IYooiIhMQQRWRiKsZFXUi+B5VKLXA1RERNF0MUkYnxaNcMNlZSFBaVIiXjD6HLISJqshiiiEyMRCKGv0f57YnO8pIeEZFgGKKITNBfUx0wRBERCYUhisgEVYSolIw8PFCUCFwNEVHTxBBFZIJaNrNCu1a2UGvKB5gTEVHDY4giMlEVZ6POJHH2ciIiITBEEZmoihB1LikbGo1G4GqIiJoehigiE9XVrQXMpRLkPijGzTsPhC6HiKjJYYgiMlHmUgm6dW4JoPxsFBERNSyGKCITJvcsny/qDKc6ICJqcAxRRCYs0Kv8PnpXbuSiqLhM4GqIiJoWhigiE9a6pQ2cmlujTKVGQup9ocshImpSGKKITJhIJEIgZy8nIhIEQxSRiQvw+jNEcXA5EVGDYogiMnHdOreERCzCnfsK3LmvELocIqImgyGKyMRZW0rh3bE5AJ6NIiJqSAxRRI1AAMdFERE1OIYookagIkRdvHYPpWVqgashImoaGKKIGoGOre3RTGaBhyUqJN7MEbocIqImgSGKqBEQi0Xa2ct5SY+IqGHUOUTdvXsX58+f12MpRFQfAX/OXs7B5UREDUPnEJWbm4tp06YhNDQUU6dOBQDs2bMH7777rr5rIyIdyD0cIRIBN24/QO6Dh0KXQ0TU6Okcot577z04Ojri0KFDkEqlAIAePXrg999/13txRFR79jILdGrTDABwjmejiIgMTucQdfz4cURHR8PZ2RkikQgA0KJFC+TkcDArkdA41QERUcPROURJJBKIxZV3KywshK2trd6KIqK6qQhR55LvQaXWCFwNEVHjpnOI6t69Oz7//PNKyzZt2oQePXrorSgiqhvP9g6wtjRDgbIEqbf+ELocIqJGTecQ9dprr+HAgQMIDQ2FQqHA4MGDsWPHDrzyyiuGqI+IdGAmEcPPvXyqgzO8pEdEZFA6hyhnZ2fs2LEDb7/9NubPn485c+Zg586dcHJy0qmdmJgYREREwMfHB1FRUdVul5GRgbFjxyI4OBhBQUEYP348Tp8+rWvZjx27b9++kMvliIyMRH5+/mPblJSUYOjQoQgJCanXsYgaWqDXn5f0OLiciMigdA5Ru3btgrm5OQYNGoQZM2Zg1KhRsLGxwe7du3Vqx8nJCXPmzMG4ceNq3M7BwQGffPIJjh8/jlOnTmHatGl48cUXUVJSomvpAIAjR45g9erVWLduHQ4fPgyxWIzFixc/tt369evRokWLOh2DSEjyP8dFJaXlolBZt/cJERE9mc4h6p133qly+ZIlS3RqZ/DgwQgLC4ODg0ON28lkMrRv3x5isRgajQZisRgPHjxAXl4eAECj0WDz5s0YMmQIgoODMWPGDGRlZVXbXmxsLCIiItC1a1fIZDJERUVh//79KCgo0G5z48YN7NmzBzNnztTpOREZAycHa7R1lkGtAS6k3Be6HCKiRstM1x00msc/8fPHH39opzswlAEDBiA7OxtlZWWIiIiAs3P57MwxMTHYuXMnNm3aBGdnZ6xZswZRUVHYtm1ble2kpKSgX79+2scdOnSAVCrF9evX4efnBwCIjo7Ga6+9BktLy3rXrVKpoFKp6t3O39t89F8SljH2h7+7IzLuFuJ0YhZ6+jgLXU6DMsb+aMrYH8aHfVIzXV6XWoeo0NBQiEQiFBcXo3///pXW5eXl4amnnqr1Qevi4MGDKC4uxu7duysFtm3btmHhwoVo06YNACAyMhL+/v64ffs2Wrdu/Vg7SqUSdnZ2lZbZ2tpCoVAAAOLi4iCTydC/f3+cOHGi3nUnJyfXu43qJCQkGKxt0p0x9Ye9WfmM5Scv38a5zmqD/5FjjIypP4j9YYzYJ/VX6xA1f/58aDQaREdH4+WXX9YuF4lEcHR0RM+ePQ1S4KMsLCwQERGBwYMHw9vbG15eXsjMzERUVFSluavEYjGysrIQHx+PL7/8EgAQGBiIjRs3wtrautKlO6B8nisbGxvk5+fj888/R0xMjN5q9vDwgLW1td7aA8pTckJCAnx9fSGRSPTaNunOGPvDu6sKP/z+Mx4oVWjh0hntWjWdedyMsT+aMvaH8WGf1EypVNb6BEitQ9To0aMBAO3atUNQUFDdKtOT0tJSZGRkwMvLCy4uLliyZEmV81QFBARg9uzZlZa5u7sjMTER4eHhAIC0tDSUlJTAzc0NV65cQXZ2NsaOHas9TkFBAUJCQrBlyxa4u7vrXKtEIjHYN6kh2ybdGVN/WEsk8OnUEmeTsnE+5T46ujYTuqQGZ0z9QewPY8Q+qZour4nOA8srApRSqURGRkalL12UlZWhuLgYZWVlUKvVKC4uRmlp6WPbHTt2DBcvXkRZWRmKioqwZs0a/PHHH+jWrRsAYMKECfjss8+Qnp4OAMjPz8eePXuqPW5ERARiY2Nx5coVKBQKrFy5EoMGDYKtrS3kcjkOHjyIuLg4xMXFYdmyZWjWrBni4uLQsWNHnZ4fkdAC/pzq4CynOiAiMgidB5bfunUL//rXv3DhwoXH1iUmJta6nbVr12LNmjXax/v27cPo0aPx4YcfQi6XY8OGDQgKCoJCocCyZctw+/ZtmJubw9PTExs2bNAOLJ80aRLEYjFmzZqFu3fvws7ODr1798awYcOqPG5ISAgiIyMxc+ZMKBQKhISE4L333gMAmJubw9HRUbutvb09xGJxpWVEpqLiFjCXr+fgYUkZLM11frsTEVENdP6p+t5778HBwQE//fQTJk2ahJiYGKxcuRJPP/20Tu1ERkYiMjKyynXnzp3T/j8sLAxhYWHVtiMWizFp0iRMmjSp1seeOHEiJk6c+MTtevTogSNHjtS6XSJj0sZJBkcHK9zLK8Kl1BwEeTetT+kRERmazpfzzp8/jw8++ABdunSBSCSCt7c3li5dii1bthigPCKqK5FIpD0bxUt6RET6p3OIUqlUaNasGQDA0tISRUVFcHZ21o5JIiLjoQ1RvI8eEZHe6Xw5r02bNkhKSoKnpyc6d+6M7777Dra2ttpgRUTGw8/dEWKxCJn3CpGVo0CrFjZCl0RE1GjofCZq5syZuHfvHgBgzpw5WL16NaKjozFv3jy9F0dE9WNjJYVX+/JbK/GGxERE+qXTmSiNRoPu3btr73cXHByMEydOoLS0VO8TShKRfgR4OeHKjVycTcrG0705VQcRkb7odCZKo9FgwIABle4rI5VKGaCIjFjFuKgLKfdRplILXA0RUeOhU4gSi8VwcXGBUqk0VD1EpGedXJvBzsYcRcVluHozV+hyiIgaDZ3HRL388st4++23kZaWpp1tvOKLiIyPWCyC3INTHRAR6ZvOn8575ZVXAAC//vrrY+t0mbGciBpOgJcTDp27hbNJ2Zg8rIvQ5RARNQo6h6itW7caog4iMiC5Z/mti1Jv5SOv4CEcbC0FroiIyPTpHKKCg4MNUQcRGZCDrSXcXO1xPTMf55LuYWBQW6FLIiIyeTqPiSIi0xToVT4uivNFERHpB0MUURMhf+Q+emq1RuBqiIhMH0MUURPh1b45rCzM8EBRguuZ+UKXQ0Rk8hiiiJoIqZkY3Tq3BACcSborcDVERKaPIYqoCflrXNQ9gSshIjJ9On86z8vLCyKR6LHl5ubmaN26NcLDwzF9+nRIpVK9FEhE+lMxLirxZi4URaWwseL7lIiornQOUa+//jq2bduGyZMnw9XVFZmZmfjmm28wZswYmJmZYfPmzXj48CGioqIMUS8R1UOrFjZwdbRB5j0FLqTcQ+9urYUuiYjIZOkconbt2oV169ahY8e/7gbfu3dvvPrqq/jxxx8RGBiI+fPnM0QRGakAL2dk3ruOs0nZDFFERPWg85ioGzduoG3byhP1tWnTBtevXwcA+Pr6IjeXNzklMlYBj0x1oNFwqgMiorrSOUR17NgRGzZsqLTsq6++0p6ZysrKgkwm0091RKR3Pm4tIDUT415eEW5lFwpdDhGRydL5ct4777yDGTNm4Ntvv4WLiwvu3LmD0tJSbbC6ceMGZs+erfdCiUg/LC3M0LVjC5xPuYezSdlo62wrdElERCZJ5xDVrVs3/Prrrzhw4ACys7Ph7OyMAQMGwNa2/Adxr1690KtXL70XSkT6E+DlpA1RI/t1ErocIiKTpHOIAgBbW1uMHDlS37UQUQMJ8HLCVzsv49K1+yguVcFCKhG6JCIik6NziFKr1dixYwcuXrwIhUJRad3HH3+st8KIyHDaOduihb0lcvIf4nJqDgL+nISTiIhqT+eB5dHR0fjwww+Rm5sLiURS6YuITINIJKr0KT0iItKdzmeifv75Z3z//ffo0KGDAcohooYS4OWE/SfTcTbpLgAfocshIjI5Op+JkkqlaNOmjSFqIaIG5O/uCLEIyLhbiOw8pdDlEBGZHJ1D1Pjx4xETE2OIWoioAcmszeHZvjkA4Bwv6RER6Uzny3lHjx7FxYsX8e2338LJqfJg1G+//VZvhRGR4ck9nZB4Mxdnk7IxpGcHocshIjIpOoeo3r17o3fv3oaohYgaWKCXE/7981WcT76HMpUaZhKdT04TETVZOoeouXPnGqIOIhJApzbNYGstRYGyFElpeejq1kLokoiITEat/ux89CalarW62i8iMi0SsQhyj/LL8hwXRUSkm1qdiQoMDMTZs2cBAF26dIFIJKpyu8TERP1VRkQNQu7phN/OZ+JMUjYmPu0tdDlERCajViFq/fr12v9v3bpVLweOiYlBbGwskpOTMWjQIHz22WfVbnvy5EksXboUGRkZ6Ny5M9577z14eXnV69hffvklCgsL0adPHyxbtgz29vaVtikpKUF4eDgKCgpw5MiROh+LyNhVzFaeeusP5BcWw15mIXBFRESmoVaX84KCgrT/Dw4OrvZLF05OTpgzZw7GjRtX43Z5eXmYM2cOpk+fjlOnTmHEiBF48cUXUVJSotPxKhw5cgSrV6/GunXrcPjwYYjFYixevPix7davX48WLTg+hBq/5naW6NjaDhoNcC75ntDlEBGZjDp9FCcjIwN79+7Fjz/+WOlLF4MHD0ZYWBgcHBxq3G7//v1o164dRo0aBXNzc0ydOhVqtRpHjx4FUD5ea/PmzRgyZAiCg4MxY8YMZGVlVdtebGwsIiIi0LVrV8hkMkRFRWH//v0oKCjQbnPjxg3s2bMHM2fO1Ok5EZmqilvAcFwUEVHt6fzpvG3btuHdd9+Fvb09rKystMtFIhHGjBmj1+IAIDk5Gd7ef43TEIlE8PT0RHJyMvr374+YmBjs3LkTmzZtgrOzM9asWYOoqChs27atyvZSUlLQr18/7eMOHTpAKpXi+vXr8PPzA1B+f8DXXnsNlpaW9a5fpVJBpVLVu52/t/novySsxtAf/u4t8dPBazh68TZy8ouELqd+NBqY4SG8vEtgaWEudDVNXmN4fzQ27JOa6fK66Byi1q9fj5UrV2Lw4MG67lonSqXysfFKtra2UCgUAMpD3cKFC7W3oomMjIS/vz9u376N1q1bV9menZ1dte3FxcVBJpOhf//+OHHiRL3rT05Orncb1UlISDBY26Q7U+6PMpUGVhZiFBWrcCHlvtDl6MWm2BMI8bYVugz6kym/Pxor9kn96RyiCgoKGixAAYC1tXWlS20AUFhYCBsbGwBAZmYmoqKiIBb/dWVSLBYjKysL8fHx+PLLLwGUf8Jw48aNNbaXn5+Pzz//XK+3tfHw8IC1tbXe2gPKU3JCQgJ8fX0hkUj02jbprrH0xyft3JGS8YfQZdTbjdv52PHbDRxJVGBSeA/Y2fBslJAay/ujMWGf1EypVNb6BIjOISo0NBQnT57UeSB5XXl4eOD777/XPtZoNEhKSsKECRMAAC4uLliyZAl69Ojx2L4BAQGYPXt2pWXu7u5ITExEeHg4ACAtLQ0lJSVwc3PDlStXkJ2djbFjxwIASktLUVBQgJCQEGzZsgXu7u461y+RSAz2TWrItkl3pt4f7VrZo10r+ydvaORKSstw/OIt3P2jFD/8NwWzRncTuiSC6b8/GiP2SdV0eU10HljevHlzvPTSS3jrrbewatWqSl+6KCsrQ3FxMcrKyqBWq1FcXIzS0tLHths0aBDS0tKwY8cOlJSU4OuvvwYA7a1nJkyYgM8++wzp6ekAgPz8fOzZs6fa40ZERCA2NhZXrlyBQqHAypUrMWjQINja2kIul+PgwYOIi4tDXFwcli1bhmbNmiEuLg4dO3bU6fkRkTAkYhGGBJSHwb1Hb+JWdsET9iAiqhudz0RdvXoVXl5eSE9P1wYXANVOwFmdtWvXYs2aNdrH+/btw+jRo/Hhhx9CLpdjw4YNCAoKgoODA7744gu8++67WLRoEdzd3bF27VqYm5efop80aRLEYjFmzZqFu3fvws7ODr1798awYcOqPG5ISAgiIyMxc+ZMKBQKhISE4L333gMAmJubw9HRUbutvb09xGJxpWVEZPzcWlkiyNsJpxOzsWXXFbw97fEz1URE9SXSPHpPlydQqVS4fPkyvLy8tCGGqqZUKpGYmAhvb2+DjIk6f/48/P39eSrWCLA/jEtFf7Rs3RnzVhyCWq3B+y+GwLdzS6FLa5L4/jA+7JOa6fL7W6fLeRKJBJMnT4ZUKq1XgUREhtbGSYahPdsDADbtvAS1utZ/LxIR1YrOY6Lat2+P7GxOyEdExu/5IV6wtjRD6q18/O9shtDlEFEjo3OImjRpEqKionD06FGkpaUhIyND+0VEZEzsZRYY+5QHAGDrnkQ8LCkTuCIiakx0Hlj+9ttvAwCmTZumHUyu0WggEomQmJio3+qIiOopvK8b9h69gey8Iuw4lIrnBnkKXRIRNRI6h6j//ve/hqiDiMggzKUSTBneBctjzuDHAykY3KM9HOzqf0snIiKdQ5Srq6sh6iAiMpi+/q6I/+06ktLzELPvKiLH+QtdEhE1AjqHKADIzc3FxYsXkZOTg0dnSDDEDYiJiOpLJBLhn+E+eG3NYfx6Mg3P9HVDBxe7J+9IRFQDnUPU8ePHMXfuXIhEIigUCtjY2ECpVKJVq1YMUURktLw7NkdIt9Y4cvE2voq/hKWzegtdEhGZOJ0/nbdixQpMnToVp06dgo2NDU6dOoV//OMfmDZtmiHqIyLSmynDu8BMIsK55Hs4c/Wu0OUQkYnTOUTduHEDs2bNAgDtpbw5c+Zg06ZN+q2MiEjPXFraYEQfNwDAVzsvQ6VSC1wREZkynUOUmZmZNjzZ2toiNzcXUqkUeXl5ei+OiEjfngvzgK21FOlZBdh/Mv3JOxARVUPnEOXp6YkzZ84AAORyOd59911ER0ejY8eOei+OiEjfZNbmGD+4fK6ob/ddhfJhqcAVEZGp0jlEvfXWW2jZsvxGnq+++ioePHiAa9euITo6Wt+1EREZxNO9OqJ1Sxv8UViMHw+kCF0OEZkonT+d5+7urv2/i4sLx0IRkcmRmokxdURXvL/lJHYcSsXTvTrC0cFK6LKIyMTofCYKADIyMrBu3TosWbIEAJCWlobU1FS9FkZEZEg9fVqhq1sLlJSpsXXvFaHLISITpHOIOnbsGMLDw3Hy5EnExcUBAO7du4ePPvpI37URERlM+QScXQEA/ztzCykZ/HAMEelG5xD1ySefYPny5fjqq69gZlZ+NdDHxwdXrvAvOSIyLe5tHdA/sA0AYFP85Up3YCAiehKdQ1RaWhrCwsIAlP8lBwCWlpYoLi7Wb2VERA1g8tNdYG4mxuXrOTh+KUvocojIhOgcopycnJCWllZpWWpqKlq1aqW3ooiIGoqjgxVG9e8MANi86zJKyzgBJxHVjs4hasyYMZg/fz6OHDkCtVqN06dP46233sK4ceMMUR8RkcE9O6Azmtla4M59BfYevSF0OURkInQOUVOnTsXAgQPx8ssvo7CwENOnT4e/vz8mTpxoiPqIiAzO2lKKF4Z4AQC+25+EQmWJwBURkSnQOUSJxWJERkbi9OnTOHLkCE6ePImFCxfi+PHjhqiPiKhBDApuh3atbFGgLMX3vyYLXQ4RmYA6zRNVoUWLFjA3N0dpaSmmTZumr5qIiBqcRCLGtGfKpzzY9ft13LmvELgiIjJ29QpRj+JHg4nI1AV6OUPu4YgylQZf7+a0LURUM72FqIrpDoiITNm0cB+IRcCRi7dx5UaO0OUQkRHTW4giImoMOrjYYVCP9gCATfGXoFbzLDsRVa3WNyBetWpVtevUas6rQkSNxwtDvPDbuVtITv8Dh89nIjSgjdAlEZERqnWIOn36dI3rg4KC6l0MEZExcLCzxLMD3BGz7yq27rmCXr4uMJdKhC6LiIxMrUPUN998Y8g6iIiMysjQTth77Cay84oQf/g6xgx0F7okIjIyHBNFRFQFS3MzTB7mDQDY/t9k5Bfy/qBEVBlDFBFRNfoHtEWnNvZQPizDv3++KnQ5RGRkGKKIiKohFovwz3AfAMC+42nIuFsgcEVEZEwYooiIauDbqSV6dG0FtVqDr3ZeFrocIjIiDFFERE/wj2e6QiIW4XTiXVxIvid0OURkJBiiiIiewNVRhqd7dwAAbNp5CSpOwElEEDBEyeXySl9dunTB7Nmzq93e09MT/v7+2u2nT59er+Pv27cPYWFh8PPzw5QpU5CZmaldFxsbC29v70r1xcfH1+t4RGTaxg/yhI2lGW7cfoCDp9OFLoeIjECt54nSt3Pnzmn/r1Kp0L9/fzz99NM17vPTTz+hU6dO9T52amoq3njjDaxevRpBQUFYsWIF5s+fj+3bt2u38fX1xQ8//FDvYxFR42Avs8C4ME9s3nUZ3+xNRB8/V1haCPYjlIiMgFFczjt8+DCUSiWGDBlS5zZ27NiBESNGICgoCM8//zxSUlKq3TY+Ph59+/ZFnz59YGlpiXnz5uHq1as17kNE9EzfjnBubo3cB8X4z/+uCV0OEQnMKP6Mio2NxfDhw2FpaVnjdlOmTIFarYaPjw9effVVuLuXzyB84MABrFq1CmvXrkXnzp2xfft2zJ49G3v37oW5uflj7SQnJ8PX11f7WCaToV27dkhJSdG2mZSUhJ49e8LW1haDBw9GZGTkE+urikqlgkql0nm/J7X56L8kLPaHcTFkf4hFwORhXlgecxY/HbyGsO5t0dxe958LTQnfH8aHfVIzXV4XwUNUbm4uDhw4gG+//bbG7b755hv4+/ujpKQEGzZswLRp07B3717IZDJs27YN06dPh6enJwBg/Pjx2LhxIy5cuIDu3bs/1pZSqYSdnV2lZba2tlAoFACA7t27Y9euXXB1dcXNmzexcOFCLF++HIsWLdL5+SUnJ+u8T20lJCQYrG3SHfvDuBiqP6w1GrRtaY6M+yX4fNtRjOrZ3CDHaWz4/jA+7JP6EzxE7dy5E+3bt4efn1+N2wUHBwMAzM3NERUVhfj4eJw9exb9+vVDZmYmli9fjhUrVmi3Ly0txd27dxEfH4/FixcDAFq3bo3du3fD2toaBQWVJ80rLCyEjY0NAKBt27ba5W5ubliwYAEWLFhQpxDl4eEBa2trnferiUqlQkJCAnx9fSGR8KaoQmN/GJeG6I+5zfOw8IsjuHBDicnhgXBrbW+Q4zQGfH8YH/ZJzZRKZa1PgAgeomJjYxEREaHzfiKRCBpN+ceMXVxcMH369GrbCQ8Pr/TYw8MDiYmJ2scKhQLp6enaS3l/JxaLtcfSlUQiMdg3qSHbJt2xP4yLIfuji1tL9PV3xeHzmfh6dyLendUbIpHIIMdqLPj+MD7sk6rp8poIOrD88uXLuHbtGkaOHFnjdikpKbh8+TLKyspQVFSE1atXo7i4GHK5HAAwYcIErF+/HlevXoVGo4FCocCBAwdQWFhYZXvh4eE4fPgwjh49iuLiYqxevRqenp7aEHXo0CFkZ2cDADIyMvDpp58iLCxMj8+ciEzd5GHeMJOIcSHlPk4n3hW6HCISgKBnomJjYxEaGoqWLVs+tk4ul2PDhg0ICgpCTk4OoqOjkZWVBQsLC/j4+GDTpk3acU1hYWF4+PAhFi5ciFu3bsHKygqBgYHaS4B/16lTJ7z//vtYtGgR7t+/Dz8/P6xcuVK7/vjx43jzzTehUCjg4OCAIUOGYN68eQZ5DYjINLVqYYPwvm6I/d81bN51GQGeTpBIjOIDz0TUQESaul6nohoplUokJibC29vbIGOizp8/D39/f56KNQLsD+PSkP1RWFSKWR/8igeKErz4bDcM693RoMczRXx/GB/2Sc10+f3NP5uIiOpIZiXF84PLPxX87b6rUBSVClwRETUkhigionoY0qsDXB1leKAowfb/Gm5KEyIyPgxRRET1YCYRY9ozXQEA8Yev426uUuCKiKihMEQREdVT9y7O6Na5JUrL1Ni654rQ5RBRA2GIIiKqJ5FIhGnPdIVIBPx2LhPJ6XlCl0REDYAhiohIDzq1aYaBQeV3O9i441KdJ+glItPBEEVEpCeTnvaGhbkEiTdzcfTiHaHLISIDY4giItKTFvZWGB3aGQCwZfdllJbV/m7wRGR6GKKIiPQoYkBnONhaICtHid1HbghdDhEZEEMUEZEeWVmYYeLT3gCA7/Yn44GiROCKiMhQGKKIiPTsqe7t0MHFDoqiUny/P0nocojIQBiiiIj0TCIWaSfg3H3kBm7fKxS4IiIyBIYoIiIDkHs6IcjbGSq1Blt2cwJOosaIIYqIyED+MaILxGIRjiXcQULqfaHLISI9Y4giIjKQdq3sMKRHewDAV/GXoFZzAk6ixoQhiojIgJ4f4gUrCzNcu5WPQ+duCV0OEekRQxQRkQE1s7XA2KfcAQBb9ySiuJQTcBI1FgxRREQGFt6vExwdrHD/jyLsOJQqdDlEpCcMUUREBmYhlWDysC4AgB8PJCOv4KHAFRGRPjBEERE1gH7+rnBv2wxFxSp8u++q0OUQkR4wRBERNQCxWIR/hvsAAPafSEPanQcCV0RE9cUQRUTUQLq6tUAvXxeoNcBXuy4LXQ4R1RNDFBFRA5o6ogvMJCKcvZqNs0nZQpdDRPXAEEVE1IBat5RhWEhHAMDmnZeh4gScRCaLIYqIqIGNH+QJmZUUN+88wK8n04Uuh4jqiCGKiKiB2VqbY/xgTwDAt/sSoXxYKnBFRFQXDFFERAIY1rsjXFraIK+gGLEHrwldDhHVAUMUEZEApGZiTB1ePgHnfw6l4v4fRQJXRES6YogiIhJIL18XdHVrgZJSFb7Zmyh0OUSkI4YoIiKBiEQiTHumKwDgwOkMXLv1h7AFEZFOGKKIiATk0c4BofI2AICv4i9Do+GUB0SmgiGKiEhgk4d5Q2omRkLqfZy8nCV0OURUSwxRREQCc2pujVGhnQAAm3ddRplKLXBFRFQbDFFEREZgzEB3NJNZIPOeAnuP3hS6HCKqBYYoIiIjYG0pxfNDyifg3PZLEgqLOAEnkbETLES9/vrr8PHxgVwu137dvn272u2Tk5Mxbtw4+Pn5YdiwYTh27Fi9jr9v3z6EhYXBz88PU6ZMQWZmpnZdbGwsvL29K9UWHx9fr+MRET3J4B7t0dbZFgXKEvzwa7LQ5RDREwh6Jmrq1Kk4d+6c9qt169ZVbldaWorZs2dj4MCBOHXqFObOnYu5c+ciJyenTsdNTU3FG2+8gejoaJw4cQKenp6YP39+pW18fX0r1RYeHl6nYxER1ZZEItZOebDz8HVk5SgEroiIamImdAG1cfLkSTx8+BAzZ86EWCzGsGHDsHXrVuzbtw8vvPACAGDHjh3YsGEDsrKy4OHhgSVLlsDd3b3K9uLj49G3b1/06dMHADBv3jz06tULKSkp1e5DRNQQAr2c4O/uiPMp97B512VMe8ZH6JLqRaVSoaiEA+WpcRI0RP3www/44Ycf0KpVK0yePBljxoypcruUlBR4eHhALP7rxJm3tzeSk8tPdx84cACrVq3C2rVr0blzZ2zfvh2zZ8/G3r17YW5u/lh7ycnJ8PX11T6WyWRo165dpRCVlJSEnj17wtbWFoMHD0ZkZCQsLS11fo4qlQoqlUrn/Z7U5qP/krDYH8alMfTH1BHeiFp5D0cv3sHRi3eELqfeRCLghUIZnh3gDpFIJHQ5TV5jeI8Yki6vi2AhatKkSXjttddgZ2eH06dP4+WXX4atrS2GDBny2LYKhQJ2dnaVltnZ2WnHMW3btg3Tp0+Hp2f5oMzx48dj48aNuHDhArp37/5Ye0ql8rH2bG1toVCUnzrv3r07du3aBVdXV9y8eRMLFy7E8uXLsWjRIp2fZ0XQM4SEhASDtU26Y38YF1Pvjz5dbHEiqRCmPvemBkCZSoOYfclIvn4bQwPsIRYzSBkDU3+PGAPBQlTXrl21/+/ZsydeeOEF7Nu3r8oQZWNjg4KCgkrLCgoKYGNjAwDIzMzE8uXLsWLFCu360tJS3L17F/Hx8Vi8eDEAoHXr1ti9ezesra0fa6+wsFDbXtu2bbXL3dzcsGDBAixYsKBOIcrDwwPW1tY671cTlUqFhIQE+Pr6QiKR6LVt0h37w7g0lv7w9xe6Av1QqVRYv/0ofj6bj5PJhTCzsMX88f4wl5pu35i6xvIeMRSlUlnrEyBGMyZKLBZXe7sDd3d3bNiwAWq1WntJLzExESNGjAAAuLi4YPr06YiIiKhy/78PCvfw8EBi4l83+1QoFEhPT692PFRNtT2JRCIx2DepIdsm3bE/jAv7w3j08rKFj5cbVn1/HkcT7uCBsgRv/aMHZFZSoUtr0vgeqZour4lgn87bs2cPCgsLoVarcfr0acTExGDQoEFVbhscHAwLCwts3LgRJSUl2Lt3L5KTkzF06FAAwIQJE7B+/XpcvXoVGo0GCoUCBw4cQGFhYZXthYeH4/Dhwzh69CiKi4uxevVqeHp6akPUoUOHkJ2dDQDIyMjAp59+irCwMAO8CkRETUNff1dEz+gFa0szXErNwetrDuP+H0VCl0VUL4Kdifr222/xzjvvQKVSoXXr1pg/fz6GDx+uXT98+HDMmjUL4eHhkEqlWLt2Ld5++22sWbMGrq6uWLNmDVq0aAEACAsLw8OHD7Fw4ULcunULVlZWCAwMRHBwcJXH7tSpE95//30sWrQI9+/fh5+fH1auXKldf/z4cbz55ptQKBRwcHDAkCFDMG/ePIO+HkREjZ2fuyM+fKkPojccQ1pWAV79/DdEz+yF9q3snrwzkRESaXjLcINQKpVITEyEt7e3QcZEnT9/Hv7+/jwVawTYH8aF/WFcquqP7Fwl3ll/DJn3CmFjJcWiaT3Q1a2FwJU2HXyP1EyX39+87QsRETUop+bW+DiyL7w7NIeiqBSLvjyKoxerv2MFkbFiiCIiogZnZ2OOd2f3Ro+urVBapsaHW09h9+/XhS6LSCcMUUREJAgLqQRvTOmOob06QKMB1v0nAVv3XKnzp6GJGhpDFBERCUYiEWPOs93wwlAvAMD2/6Zg1ffnUKbirWLI+DFEERGRoEQiEcYP8kTkOH+IxSL891QG3v3qBIqKy4QujahGDFFERGQUBvdoj7f/EQxzqQRnr2bjzbVH8EdBsdBlEVWLIYqIiIxG9y6t8P6LvWFrbY5rGX/gtdWHcee+QuiyiKrEEEVEREbFs31zLJ/XF87NrXEnR4FXV/+GlIw8ocsiegxDFBERGR1XRxmWR/aFm6s98gtL8Ob/HcGZq3eFLouoEoYoIiIySg52lvhgTgj8PRzxsESFdzedwH9PpQtdFpEWQxQRERkta0sp3vlnT/QPaAOVWoOV353D9v8mcy4pMgoMUUREZNSkZmJETQjAswM6AwC27knEl/9JgErNIEXCYogiIiKjJxaLMHVEV8wY5QORCNh95AY+2noKJaUqoUujJowhioiITEZ43054dWIQzCRiHEu4g3fWH0OhskTosqiJYogiIiKT0tffFUtn9oKNpRkuX8/Ba2t+x728IqHLoiaIIYqIiEyOb+eW+HBuXzS3s0TG3QK8uvo3pN15IHRZ1MQwRBERkUnq4GKH5fP6oq2zDDn5D7FwzWEkpN4XuixqQhiiiIjIZDk5WOOjuX3h3aE5FA/L8M6Xx3Dkwm2hy6ImgiGKiIhMmq21Od6d3Ru9fF1QplLjo29OYefh60KXRU0AQxQREZk8C6kECyd3x7DeHaDRAOvjErBl12VOykkGxRBFRESNgkQswuyIbpj4tBcA4KeD1/DZtrMoU6kFrowaK4YoIiJqNEQiEZ4L88TLz8khFotw8MwtLN14HMqHpUKXRo0QQxQRETU6YcHtsGhaD1iYS3Au+R7eXHsEeQUPhS6LGhmGKCIiapSCvJ3x/oshsJeZI/VWPl5bfRi37xUKXRY1IgxRRETUaHm0c8DHkX3RqoU1snKUeHX1YSSn5wldFjUSDFFERNSotW4pw8eRfdG5jT0eKErw5tojOJ14V+iyqBFgiCIiokbPwdYS78/pgwBPJxSXqPDuVyfw68k0ocsiE8cQRURETYKVhRkW/bMHBga1hVqtwarvz+P7/UmcS4rqjCGKiIiaDDOJGPPHyzH2KXcAQMy+q1gbexEqNYMU6Y4hioiImhSRSITJw7pg1mhfiETA3qM38eHXJ1FcqhK6NDIxDFFERNQkjejjhoWTukNqJsbxS1lYtO4oCpQlQpdFJoQhioiImqwQv9ZYOrMXbKykSLyZi4VrDiM7Tyl0WWQiGKKIiKhJ8+nUEh/N7YOW9pbIuFuIVz8/jBu384Uui0wAQxQRETV57VvZ4ePIfmjXyha5Dx7i9S9+R8K1+0KXRUaOIYqIiAiAo4MVPnqpD7q6tYDyYRneWX8Mh89nCl0WGTFBQlRJSQneeustDBw4EHK5HMOHD8fOnTur3d7T0xP+/v6Qy+WQy+WYPn16vY6/b98+hIWFwc/PD1OmTEFm5l9vktjYWHh7e2uPJZfLER8fX6/jERGRaZBZm2PpzF7o3c0FZSo1lsecRvxvqUKXRUbKTIiDlpWVwcnJCV9//TXatGmDM2fOYNasWWjTpg3kcnmV+/z000/o1KlTvY+dmpqKN954A6tXr0ZQUBBWrFiB+fPnY/v27dptfH198cMPP9T7WEREZHrMpRK8Nqk7NsYlYNeRG9iw4xLu5z/E1OFdIBaLhC6PjIggIcra2hovv/yy9nFQUBACAgJw7ty5akNUTXbs2IENGzYgKysLHh4eWLJkCdzd3avcNj4+Hn379kWfPn0AAPPmzUOvXr2QkpJS7T5ERNS0SMQizBzti+b2lti6JxH/+d815PxRhN5+rYUurd7E0EBVoha6jEZBkBD1d0qlEpcuXcLkyZOr3WbKlClQq9Xw8fHBq6++qg08Bw4cwKpVq7B27Vp07twZ27dvx+zZs7F3716Ym5s/1k5ycjJ8fX21j2UyGdq1a1cpRCUlJaFnz56wtbXF4MGDERkZCUtLyzo9N5VKBZVKvxO4VbSn73apbtgfxoX9YVxMvT8i+neCg60F1my/gN/OZ+K3RjJGytxMhEu3ExDerxOcm1sLXY5R0eV7VaQR+KZBarUa8+fPx8OHD/Hll19CJHr8VOnJkyfh7++PkpISbNiwAbGxsdi7dy9kMhlmzJiBAQMG4Pnnn9duHxYWhg8++ADdu3d/rK0pU6Zg0KBBmDhxonbZ+PHj8eyzz2Ls2LHIyMgAALi6uuLmzZtYuHAhunXrhkWLFun0vJRKJRITE3Xah4iIjNP1rIc4kliA0jLTvz1MYZEauYVlAACRCPBua4VeXjK0bWkhcGXGxdvbG9bWNQdMQc9EaTQaLF68GNnZ2di0aVOVAQoAgoODAQDm5uaIiopCfHw8zp49i379+iEzMxPLly/HihUrtNuXlpbi7t27iI+Px+LFiwEArVu3xu7du2FtbY2CgoJK7RcWFsLGxgYA0LZtW+1yNzc3LFiwAAsWLNA5RFXw8PB4YifoSqVSISEhAb6+vpBIJHptm3TH/jAu7A/j0lj6wx9AxFChq9CPsrIyxP1yGgmZwIWU+7iSXoQr6UXwbO+Akf3c0KNrK0ia8NgvpVKJ5OTkWm0rWIjSaDRYsmQJEhMTsWXLFm2IqQ2RSKS967aLiwumT5+OiIiIKrcNDw+v9NjDw6PSGSKFQoH09PRqx0OJxeJ63eFbIpEY7AeHIdsm3bE/jAv7w7iwP4xL59aWGDPMHxnZCuw4lIr/nb2FpLQ8fPzNGTg3t0Z4PzeEdW8Ha0up0KU2OF2+TwWbJ2rp0qW4cOECNm3aBJlMVu12KSkpuHz5MsrKylBUVITVq1ejuLhYOwB9woQJWL9+Pa5evQqNRgOFQoEDBw6gsLCwyvbCw8Nx+PBhHD16FMXFxVi9ejU8PT21IerQoUPIzs4GAGRkZODTTz9FWFiYnp89ERGR8Dq42OHl8XJ89fYgPBfmAVtrc9zNVWJD3CVMe/cXbNl1Gff/KBK6TKMlyJmozMxM/Pvf/4a5uTn69++vXT5r1izMnj0bcrkcGzZsQFBQEHJychAdHY2srCxYWFjAx8cHmzZtgp2dHYDy8U8PHz7EwoULcevWLVhZWSEwMFB7CfDvOnXqhPfffx+LFi3C/fv34efnh5UrV2rXHz9+HG+++SYUCgUcHBwwZMgQzJs3z5AvBxERkaAc7Cwx8WlvjHnKHQdPZ2DHb6nIvKfATwevIe5QKvr4uWJU/07o3KaZ0KUaFcEHljdWFQPLazMwTVcqlQrnz5+Hv78/T48bAfaHcWF/GBf2h/GpTZ+o1RqcTryLuEOpSEj96/Y3vp1aYlRoJwR5OzfaObN0+f1tFFMcEBERkfEQi0UI7toKwV1b4VrGH9jxWyoOn89EQup9JKTeh6ujDUb264QBQW1had50owTvnUdERETV6ty2GRa8EIiNbw3CswM6w8bSDJn3FPi/ny5i2rv7EbM3EXkPHgpdpiCabnwkIiKiWmvZzApTR3TFuDAP/HoqHfG/XcfdXCW+/zUZPx28hv4BbTAytBM6uNgJXWqDYYgiIiKiWrO2lCK8bycMD3HD8Ut3sONQKhJv5uLXU+n49VQ6/D0cMTq0M+SejtXO/9hYMEQRERGRziRiEUK6tUZIt9a4mpaLuEOpOHbxNs4n38P55Hto18oWo/p1QmhAG5hLG+eHChiiiIiIqF682jfH65ObIytHgZ2/X8f+E2lIzyrA5z+cx9Y9iRjepyOe7tUB9rLGdWsZhigiIiLSi1YtbDBjpC8mDPbCL8fTsPNwKu7nP8S3+65i+6/JGBDUFiP7dUJbZ1uhS9ULhigiIiLSK5mVFBEDOiO8nxuOXLiNuEPXcO1WPn4+noafj6chyNsZo/t3gm+nliY9boohioiIiAzCTCJGaEAb9JO74vL1HMQdSsXJK1k4nXgXpxPvws3VHqNCO6GPnyukZqY36xJDFBERERmUSCSCT6eW8OnUEpn3ChH/Wyp+PZWB65n5WPHvs9iy6wqe6euGoT3bQ2ZtLnS5tcYQRURERA3G1VGGF5/1wwtDvbHv2E3s+v06ch88xNe7r+D7/UkIC26H8L6d4NLSRuhSn4ghioiIiBqcnY05xoV5YHT/TvjtXCbiDqXi5p0H2PX7Dew+cgM9fVwwKrQTvDs0N9pxUwxRREREJBipmQRPdW+HgUFtcSHlHuIOpeLM1WwcS7iDYwl34NGuGUaFdkZvXxdIJMY1boohioiIiAQnEong7+EEfw8npGU9QPxv13HwTAaS0//Ax9+chpODFZ7p2wmDe7SDtaVU6HIB8AbEREREZGTat7JD5Dh/bHp7ECYM9oSdjTmy84qwKf4Spi79BZviLyE7Tyl0mQxRREREZJwcbC3x/BAvfLVoMOaO9UMbJxmKissQdygVM97/Fb+eTBe0Pl7OIyIiIqNmIZVgSM8OGBTcHmeTsvGf/13DxWv3kZWrELQuhigiIiIyCWKxCEHezgjydkahskTwsVEMUURERGRyjGFSTo6JIiIiIqoDhigiIiKiOmCIIiIiIqoDhigiIiKiOmCIIiIiIqoDhigiIiKiOmCIIiIiIqoDhigiIiKiOmCIIiIiIqoDhigiIiKiOmCIIiIiIqoDhigiIiKiOmCIIiIiIqoDM6ELaKzUajUAoKioSO9tq1QqAIBSqYREItF7+6Qb9odxYX8YF/aH8WGf1Kzi93bF7/GaiDQajcbQBTVFOTk5uHnzptBlEBERUR106NABLVq0qHEbhigDKSsrQ35+PiwsLCAW86opERGRKVCr1SguLoa9vT3MzGq+YMcQRURERFQHPEVCREREVAcMUURERER1wBBFREREVAcMUURERER1wBBFREREVAcMUURERER1wBBFREREVAcMUURERER1wBBFREREVAcMUUbuwYMHePnllyGXy9GnTx9s2bJFu87T0xOpqanax3FxcQgODsapU6cEqLRpYH8YF/aHcWF/GBf2h+HVfFMYEtzSpUtRUlKCw4cPIzMzE1OnTkXHjh0RGhpaabtvv/0Wn3/+OTZu3Ihu3boJVG3jx/4wLuwP48L+MC7sjwagIaOlUCg0Xbt21SQlJWmXrVixQhMZGanRaDQaDw8PzbVr1zTr1q3T9O7dW3P16lWhSm0S2B/Ghf1hXNgfxoX90TB4JsqI3bx5ExqNBh4eHtplXl5e+OWXX7SPP//8c1y4cAExMTHo2LGjEGU2GewP48L+MC7sD+PC/mgYHBNlxJRKJWQyWaVldnZ2UCgU2seHDx9Gz549+QZoAOwP48L+MC7sD+PC/mgYDFFGzNrautI3PAAUFBTAxsZG+3jFihU4duwY3n333YYur8lhfxgX9odxYX8YF/ZHw2CIMmIdOnQAAKSkpGiXJSYmwt3dXfu4bdu22Lp1K/bv349ly5Y1dIlNCvvDuLA/jAv7w7iwPxoGQ5QRs7a2xpAhQ/DZZ5+hsLAQycnJ+PHHH/Hss89W2q59+/bYunUrfv75Z7z//vsCVdv4sT+MC/vDuLA/jAv7o4EIOqydnig/P18TGRmp8ff314SEhGg2b96sXVfx6YoK169f1/Tp00fz4YcfClBp08D+MC7sD+PC/jAu7A/DE2k0Go3QQY6IiIjI1PByHhEREVEdMEQRERER1QFDFBEREVEdMEQRERER1QFDFBEREVEdMEQRERER1QFDFBEREVEdMEQRERER1QFDFBGRgcjlcpw4cULoMojIQBiiiKjRmDRpEj777DMAwMCBA7F9+/YGOW5sbCz69ev32PJz586hR48eDVIDETU8higiomqoVCqo1WqhyyAiI8UQRUSNzvTp03H79m0sXboUcrkcw4cP166Li4tDeHg4AgMDMXz4cOzevVu77sSJE/D09MTu3bsxZMgQ+Pn5IScnB3v37kVERAS6d++OHj16YPbs2cjIyAAAnD59GosXL0Z2djbkcjnkcjni4+MBAJ6enjh69Ki2/f/973+IiIhAYGAghgwZgk2bNlUKaZ6envjmm28wfvx4yOVyPPPMMzh9+rShXy4iqiuh74BMRKQvEydO1KxYsUKj0Wg0AwYM0Pzwww+V1v/000+a0NBQzcWLFzUqlUpz6tQpjVwu15w6dUqj0Wg0x48f13h4eGjmzJmjyc3N1RQXF2vKyso0hw4d0iQmJmrKyso0OTk5mlmzZmnGjRtXqd2+ffs+Vo+Hh4fmyJEjGo1Go7lw4YKma9eumt27d2tKS0s1CQkJmpCQEM3mzZsrbf/MM89obt68qSktLdUsW7ZM079/f32/TESkJzwTRURNxubNmzF79mz4+vpCLBYjKCgIw4YNw3/+859K2y1YsAAODg4wNzeHRCJBv3794OXlBYlEgubNm2PevHk4f/48CgsLa33sH3/8EaGhoRg2bBjMzMzg4+OD6dOn47vvvqu03bRp09C+fXuYmZlh3LhxuH37Nu7fv6+X509E+mUmdAFERA0lLS0NH330ET755BPtMpVKhaCgoErbtWnTptLjkydP4osvvkBqaiqUSqV2eW5uLmQyWa2OfefOHXTu3LnSsnbt2uHOnTuVljk5OWn/b2VlBQBQKBRo2bJlrY5DRA2HIYqIGiWRSPTYspYtW2LevHkYNWpUjfuKxX+dpC8pKcGsWbPw0ksv4YsvvoBMJsOVK1cwevRoaDSax7avjouLC9LT0ystS09Ph4uLSy2eDREZI17OI6JGydHREdevX6+0bMqUKfi///s/XLx4EWq1GiUlJbh48SIuXbpUbTulpaUoLi6Gvb09ZDIZ7t69i5UrV1bapmXLlsjLy0NeXl617Tz77LM4dOgQfv75Z6hUKly5cgWbNm3Cc889V6/nSUTCYYgiokZpzpw5OHDgAIKCgvDMM88AKA9Rc+fORXR0NIKDg9G3b18sX74cRUVF1bZjY2ODZcuWYe3atZDL5ZgxYwaGDh1aaZuePXviqaeewtChQxEUFISdO3c+1o6fnx9WrVqFdevWoXv37nj55ZcxadIkTJ48Wb9PnIgajEhTcT6aiIiIiGqNZ6KIiIiI6oAhioiIiKgOGKKIiIiI6oAhioiIiKgOGKKIiIiI6oAhioiIiKgOGKKIiIiI6oAhioiIiKgOGKKIiIiI6oAhioiIiKgOGKKIiIiI6oAhioiIiKgO/h8pPMmj74EmYgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# lr_scheduler.py\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def MultiStepLR(initial_learning_rate, lr_steps, lr_rate, name='MultiStepLR'):\n",
        "    # \"\"\"Multi-steps learning rate scheduler.\"\"\"\n",
        "    lr_steps_value = [initial_learning_rate]\n",
        "    for _ in range(len(lr_steps)):\n",
        "        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n",
        "    return tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "        boundaries=lr_steps, values=lr_steps_value)\n",
        "\n",
        "\n",
        "# def CosineAnnealingLR_Restart(initial_learning_rate, t_period, lr_min):\n",
        "#     # \"\"\"Cosine annealing learning rate scheduler with restart.\"\"\"\n",
        "#     return tf.keras.experimental.CosineDecayRestarts(\n",
        "#         initial_learning_rate=initial_learning_rate,\n",
        "#         first_decay_steps=t_period, t_mul=1.0, m_mul=1.0,\n",
        "#         alpha=lr_min / initial_learning_rate)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # pretrain PSNR lr scheduler\n",
        "    lr_scheduler = MultiStepLR(2e-4, [200, 400, 600, 800], 0.5)\n",
        "\n",
        "    # ESRGAN lr scheduler\n",
        "    # lr_scheduler = MultiStepLR(1e-4, [50000, 100000, 200000, 300000], 0.5)\n",
        "\n",
        "    # Cosine Annealing lr scheduler\n",
        "    # lr_scheduler = CosineAnnealingLR_Restart(2e-4, 250000, 1e-7)\n",
        "\n",
        "    ##############################\n",
        "    # Draw figure\n",
        "    ##############################\n",
        "\n",
        "    N_iter = 1000\n",
        "    step_list = list(range(0, N_iter, 100))\n",
        "    lr_list = []\n",
        "    for i in step_list:\n",
        "        current_lr = lr_scheduler(i).numpy()\n",
        "        lr_list.append(current_lr)\n",
        "\n",
        "    import matplotlib as mpl\n",
        "    from matplotlib import pyplot as plt\n",
        "    import matplotlib.ticker as mtick\n",
        "    mpl.style.use('default')\n",
        "    import seaborn\n",
        "    seaborn.set(style='whitegrid')\n",
        "    seaborn.set_context('paper')\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.subplot(111)\n",
        "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
        "    plt.title('Title', fontsize=16, color='k')\n",
        "    plt.plot(step_list, lr_list, linewidth=1.5, label='learning rate scheme')\n",
        "    legend = plt.legend(loc='upper right', shadow=False)\n",
        "    ax = plt.gca()\n",
        "    labels = ax.get_xticks().tolist()\n",
        "    for k, v in enumerate(labels):\n",
        "        labels[k] = str(int(v / 1000)) + 'K'\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1e'))\n",
        "\n",
        "    ax.set_ylabel('Learning rate')\n",
        "    ax.set_xlabel('Iteration')\n",
        "    fig = plt.gcf()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkuI8-mnHqVP"
      },
      "outputs": [],
      "source": [
        "# losses.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input, VGG19\n",
        "\n",
        "def get_pixel_loss(criterion='l1'):\n",
        "    \"\"\"pixel loss\"\"\"\n",
        "    if criterion == 'l1':\n",
        "        return tf.keras.losses.MeanAbsoluteError()\n",
        "    elif criterion == 'l2':\n",
        "        return tf.keras.losses.MeanSquaredError()\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            'Loss type {} is not recognized.'.format(criterion))\n",
        "\n",
        "def get_content_loss(criterion='l1', output_layer=54, before_act=True):\n",
        "    \"\"\"content loss\"\"\"\n",
        "    if criterion == 'l1':\n",
        "        loss_func = tf.keras.losses.MeanAbsoluteError()\n",
        "    elif criterion == 'l2':\n",
        "        loss_func = tf.keras.losses.MeanSquaredError()\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            'Loss type {} is not recognized.'.format(criterion))\n",
        "    vgg = VGG19(input_shape=(None, None, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "    if output_layer == 22:  # Low level feature\n",
        "        pick_layer = 5\n",
        "    elif output_layer == 54:  # Hight level feature\n",
        "        pick_layer = 20\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            'VGG output layer {} is not recognized.'.format(criterion))\n",
        "\n",
        "    if before_act:\n",
        "        vgg.layers[pick_layer].activation = None\n",
        "\n",
        "    fea_extrator = tf.keras.Model(vgg.input, vgg.layers[pick_layer].output)\n",
        "    fea_extrator.trainable = False\n",
        "\n",
        "    @tf.function\n",
        "    def content_loss(real_hr, fake_hr):\n",
        "        # the input scale range is [0, 1] (vgg is [0, 255]).\n",
        "        # 12.75 is rescale factor for vgg featuremaps.\n",
        "        preprocess_fake_hr = preprocess_input(fake_hr * 255.) / 12.75\n",
        "        preprocess_real_hr = preprocess_input(real_hr * 255.) / 12.75\n",
        "        fake_hr_features = fea_extrator(preprocess_fake_hr)\n",
        "        real_hr_features = fea_extrator(preprocess_real_hr)\n",
        "\n",
        "        return loss_func(real_hr_features, fake_hr_features)\n",
        "\n",
        "    return content_loss\n",
        "\n",
        "def get_discriminator_loss(gan_type='ragan'):\n",
        "    \"\"\"discriminator loss\"\"\"\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    sigma = tf.sigmoid\n",
        "\n",
        "    def discriminator_loss_ragan(real_discriminator_logits, fake_discriminator_logits):\n",
        "        real_logits = sigma(real_discriminator_logits - tf.reduce_mean(fake_discriminator_logits))\n",
        "        fake_logits = sigma(fake_discriminator_logits - tf.reduce_mean(real_discriminator_logits))\n",
        "        return 0.5 * (\n",
        "            cross_entropy(tf.ones_like(real_logits), real_logits) +\n",
        "            cross_entropy(tf.zeros_like(fake_logits), fake_logits))\n",
        "\n",
        "    def discriminator_loss(real_discriminator_logits, fake_discriminator_logits):\n",
        "        real_loss = cross_entropy(tf.ones_like(real_discriminator_logits), sigma(real_discriminator_logits))\n",
        "        fake_loss = cross_entropy(tf.zeros_like(fake_discriminator_logits), sigma(fake_discriminator_logits))\n",
        "        return real_loss + fake_loss\n",
        "\n",
        "    if gan_type == 'ragan':\n",
        "        return discriminator_loss_ragan\n",
        "    elif gan_type == 'gan':\n",
        "        return discriminator_loss\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            'Discriminator loss type {} is not recognized.'.format(gan_type))\n",
        "\n",
        "def get_generator_loss(gan_type='ragan'):\n",
        "    \"\"\"generator loss\"\"\"\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    sigma = tf.sigmoid\n",
        "\n",
        "    def generator_loss_ragan(real_discriminator_logits, fake_discriminator_logits):\n",
        "        real_logits = sigma(real_discriminator_logits - tf.reduce_mean(fake_discriminator_logits))\n",
        "        fake_logits = sigma(fake_discriminator_logits - tf.reduce_mean(real_discriminator_logits))\n",
        "        return 0.5 * (\n",
        "            cross_entropy(tf.ones_like(fake_logits), fake_logits) +\n",
        "            cross_entropy(tf.zeros_like(real_logits), real_logits))\n",
        "\n",
        "    def generator_loss(real_discriminator_logits, fake_discriminator_logits):\n",
        "        return cross_entropy(tf.ones_like(fake_discriminator_logits), sigma(fake_discriminator_logits))\n",
        "\n",
        "    if gan_type == 'ragan':\n",
        "        return generator_loss_ragan\n",
        "    elif gan_type == 'gan':\n",
        "        return generator_loss\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            'Generator loss type {} is not recognized.'.format(gan_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzYZSzR6HeQz"
      },
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DATA_PATH = \"/content/HR\"\n",
        "\n",
        "def scale_input_image(img):\n",
        "    #img/ 255.\n",
        "    return tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
        "\n",
        "# def unscale_output_image(img):\n",
        "#     #img * 255\n",
        "#     return tf.image.convert_image_dtype(img, dtype=tf.uint8, saturate=True)\n",
        "\n",
        "def random_crop_and_flip(img, random_crop_size):\n",
        "    assert img.shape[2] == 3\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "    dy, dx = random_crop_size\n",
        "    x = np.random.randint(0, width - dx + 1)\n",
        "    y = np.random.randint(0, height - dy + 1)\n",
        "    image = img[y:(y+dy), x:(x+dx), :]\n",
        "    flip_case = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    if(tf.equal(flip_case, 0)):\n",
        "        image = tf.image.flip_left_right(image)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(image_path, hr_height, hr_width, crop_per_image, ext):\n",
        "    assert ext in ['.png', '.jpg', '.jpeg', '.JPEG']\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if ext == '.png':\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "    else:\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "    image = scale_input_image(image)\n",
        "    cropped_images = [ random_crop_and_flip(image, (hr_height, hr_width)) for _ in range(crop_per_image)]\n",
        "\n",
        "    return cropped_images\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(hr_height, hr_width, scale, crop_per_image=20, ext='.png'):\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(DATA_PATH):\n",
        "        for file in files:\n",
        "            if f'{ext}' in file:\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    random.shuffle(image_paths)\n",
        "    images = []\n",
        "    for img_path in image_paths:\n",
        "        images += load_and_preprocess_image(img_path, hr_height, hr_width, crop_per_image, ext)\n",
        "\n",
        "    random.shuffle(images)\n",
        "    hr_images = []\n",
        "    lr_images = []\n",
        "    for img in images:\n",
        "        hr_image = img\n",
        "        lr_shape = [int(hr_image.shape[0]/scale), int(hr_image.shape[1]/scale)]\n",
        "        lr_image = tf.image.resize(hr_image, lr_shape, method=tf.image.ResizeMethod.BICUBIC)\n",
        "        #lr_image = lr_image / 255\n",
        "        lr_image = tf.clip_by_value(\n",
        "        lr_image, 0, 1, name=None\n",
        "        )\n",
        "        hr_images.append(hr_image)\n",
        "        lr_images.append(lr_image)\n",
        "\n",
        "    lr_dataset = tf.data.Dataset.from_tensor_slices(lr_images)\n",
        "    hr_dataset = tf.data.Dataset.from_tensor_slices(hr_images)\n",
        "\n",
        "    dataset = tf.data.Dataset.zip((lr_dataset, hr_dataset))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuxTzvTsHPrO"
      },
      "outputs": [],
      "source": [
        "# esrgan.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, PReLU, Dropout, Dense\n",
        "from tensorflow.keras.layers import BatchNormalization, Concatenate, Lambda, Add\n",
        "\n",
        "\n",
        "def residual_dense_block(input, filters):\n",
        "    x1 = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same')(input)\n",
        "    x1 = LeakyReLU(0.2)(x1)\n",
        "    x1 = Concatenate()([input, x1])\n",
        "\n",
        "    x2 = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same')(x1)\n",
        "    x2 = LeakyReLU(0.2)(x2)\n",
        "    x2 = Concatenate()([input, x1, x2])\n",
        "\n",
        "    x3 = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same')(x2)\n",
        "    x3 = LeakyReLU(0.2)(x3)\n",
        "    x3 = Concatenate()([input, x1, x2, x3])\n",
        "\n",
        "    x4 = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same')(x3)\n",
        "    x4 = LeakyReLU(0.2)(x4)\n",
        "    x4 = Concatenate()([input, x1, x2, x3, x4])\n",
        "\n",
        "    x5 = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same')(x4)\n",
        "    x5 = Lambda(lambda x: x * 0.2)(x5)\n",
        "    x = Add()([x5, input])\n",
        "\n",
        "    return x\n",
        "\n",
        "def rrdb(input, filters):\n",
        "    x = residual_dense_block(input, filters)\n",
        "    x = residual_dense_block(x, filters)\n",
        "    x = residual_dense_block(x, filters)\n",
        "    x = Lambda(lambda x: x * 0.2)(x)\n",
        "    out = Add()([x, input])\n",
        "    return out\n",
        "\n",
        "def sub_pixel_conv2d(scale_factor=2, **kwargs):\n",
        "    return Lambda(lambda  x: tf.nn.depth_to_space(x, scale_factor), **kwargs)\n",
        "\n",
        "def upsample(input_tensor, filters, scale_factor=2):\n",
        "    x = Conv2D(filters=filters*4, kernel_size=3, strides=1, padding='same')(input_tensor)\n",
        "    x = sub_pixel_conv2d(scale_factor=scale_factor)(x)\n",
        "    x = PReLU(shared_axes=[1,2])(x)\n",
        "    return x\n",
        "\n",
        "def rrdb_net(input_shape=(None, None, 3), filters=64, scale_factor=4, name='RRDB_model'):\n",
        "    lr_image = Input(shape=input_shape, name='input')\n",
        "\n",
        "    #Pre-residual\n",
        "    x_start = Conv2D(filters, kernel_size=3, strides=1, padding='same')(lr_image)\n",
        "    x_start = LeakyReLU(0.2)(x_start)\n",
        "\n",
        "    #Residual block\n",
        "    x = rrdb(x_start, filters)\n",
        "\n",
        "    #Post Residual block\n",
        "    x = Conv2D(filters,  kernel_size=3, strides=1, padding='same')(x)\n",
        "    x = Lambda(lambda x: x * 0.2)(x)\n",
        "    x = Add()([x, x_start])\n",
        "\n",
        "    #Upsampling\n",
        "    x = upsample(x, filters, scale_factor)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    out = Conv2D(filters=3, kernel_size=3, strides=1, padding='same')(x)\n",
        "\n",
        "    return Model(inputs=lr_image, outputs=out, name=name)\n",
        "\n",
        "def conv2d_block(input, filters, strides=1, bn=True):\n",
        "    x = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(input)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    if bn:\n",
        "        x = BatchNormalization(momentum=0.8)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def discriminator_net(input_shape=(None, None, 3), filters=64, name='Discriminator'):\n",
        "    img = Input(shape=input_shape)\n",
        "\n",
        "    x = conv2d_block(img, filters, bn=False)\n",
        "    x = conv2d_block(x, filters, strides=2)\n",
        "    x = conv2d_block(x, filters*2)\n",
        "    x = conv2d_block(x, filters*2, strides=2)\n",
        "    x = conv2d_block(x, filters*4)\n",
        "    x = conv2d_block(x, filters*4, strides=2)\n",
        "    x = conv2d_block(x, filters*8)\n",
        "    x = conv2d_block(x, filters*8, strides=2)\n",
        "    x = Dense(filters*16)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(1)(x)\n",
        "\n",
        "    return Model(inputs=img, outputs=x, name=name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610,
          "referenced_widgets": [
            "427b24f5b0444c75a0dd47069583d305",
            "42a16da46407469090f26457cf77623c",
            "2d7c8b967a964aa7af221bed3020a98f",
            "76c76805c7c447b795282b450ffa002a",
            "8f14f780a64f4c7e89fbfde169ab442b",
            "f2b2f67a63524f0691777d2ba4f8ea5f",
            "ffe79bf4d140477fb3a87b8cc6189181",
            "77ed74671ce2427a90c9c47cbf129509"
          ]
        },
        "id": "PXO_hLvkJw3E",
        "outputId": "943df0b6-5e11-4732-b46a-7094e297565c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] load ckpt from ./saved/checkpoints/psnr/ckpt-18 at step 7948000.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:psnr-training) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "427b24f5b0444c75a0dd47069583d305"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">psnr-training</strong> at: <a href='https://wandb.ai/personal-nag/uncategorized/runs/psnr-training' target=\"_blank\">https://wandb.ai/personal-nag/uncategorized/runs/psnr-training</a><br/> View project at: <a href='https://wandb.ai/personal-nag/uncategorized' target=\"_blank\">https://wandb.ai/personal-nag/uncategorized</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240423_200842-psnr-training/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:psnr-training). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240423_200919-psnr-training</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/personal-nag/uncategorized/runs/psnr-training' target=\"_blank\">psnr-training</a></strong> to <a href='https://wandb.ai/personal-nag/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/personal-nag/uncategorized' target=\"_blank\">https://wandb.ai/personal-nag/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/personal-nag/uncategorized/runs/psnr-training' target=\"_blank\">https://wandb.ai/personal-nag/uncategorized/runs/psnr-training</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available epochs: [8000]\n",
            "Enter epoch to resume from (or 0 to start fresh): 8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CheckpointLoadStatus' object has no attribute 'optimizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9efcc5871db3>\u001b[0m in \u001b[0;36m<cell line: 181>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-9efcc5871db3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0moptimizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADAM_BETA1_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADAM_BETA2_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Update the starting step based on the chosen epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_epoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mleng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CheckpointLoadStatus' object has no attribute 'optimizer'"
          ]
        }
      ],
      "source": [
        "#train_psnr.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "# from modules.esrgan import rrdb_net\n",
        "# from modules.lr_scheduler import MultiStepLR\n",
        "# from modules.data import load_dataset\n",
        "# from modules.losses import get_pixel_loss\n",
        "\n",
        "HAS_WANDB_ACCOUNT = False\n",
        "PROJECT = 'esrgan-tf2'\n",
        "import wandb\n",
        "if not HAS_WANDB_ACCOUNT:\n",
        "    wandb.login(anonymous='allow')\n",
        "else:\n",
        "    wandb.login()\n",
        "\n",
        "INITIAL_LR = 2e-4\n",
        "LR_RATE = 0.5\n",
        "LR_STEPS = [200, 400, 600, 800]\n",
        "ADAM_BETA1_G = 0.9\n",
        "ADAM_BETA2_G = 0.99\n",
        "W_PIXEL = 1.0\n",
        "PIXEL_CRITERION = 'l1'\n",
        "\n",
        "HR_HEIGHT = 128\n",
        "HR_WIDTH = 128\n",
        "SCALE = 4\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 10240\n",
        "INPUT_SHAPE=(None, None, 3)\n",
        "\n",
        "NUM_ITER = 1000\n",
        "SAVE_STEPS = 500\n",
        "\n",
        "\n",
        "CHECK_POINT_PATH =  \"./saved/checkpoints/psnr\"\n",
        "Path(CHECK_POINT_PATH).mkdir(parents=True, exist_ok=True)\n",
        "SAVE_MODEL_PATH = \"./saved/models/psnr.h5\"\n",
        "Path(SAVE_MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "\n",
        "    num_epochs = 0\n",
        "\n",
        "\n",
        "    dataset = load_dataset(HR_HEIGHT, HR_WIDTH, SCALE)\n",
        "    leng= len(dataset)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    model = rrdb_net(input_shape=INPUT_SHAPE,scale_factor=SCALE)\n",
        "    learning_rate = MultiStepLR(INITIAL_LR, LR_STEPS, LR_RATE)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate= learning_rate,\n",
        "                                        beta_1= ADAM_BETA1_G,\n",
        "                                        beta_2= ADAM_BETA2_G\n",
        "                                        )\n",
        "    pixel_loss = get_pixel_loss(PIXEL_CRITERION)\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(step=tf.Variable(0, name='step'),\n",
        "                                     optimizer=optimizer,\n",
        "                                     model=model)\n",
        "    manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
        "                                         directory=CHECK_POINT_PATH,\n",
        "                                         max_to_keep=3)\n",
        "    # if manager.latest_checkpoint:\n",
        "    #     checkpoint.restore(manager.latest_checkpoint)\n",
        "    #     print('[*] load ckpt from {} at step {}.'.format(\n",
        "    #         manager.latest_checkpoint, checkpoint.step.numpy()))\n",
        "    # else:\n",
        "    #     print(\"[*] training from scratch.\")\n",
        "\n",
        "    saved_checkpoints = [int(ckpt.split('-')[-1]) for ckpt in os.listdir(CHECK_POINT_PATH)]\n",
        "        if saved_checkpoints:\n",
        "            latest_epoch = max(saved_checkpoints)\n",
        "            print(f\"Found saved checkpoints. Latest epoch: {latest_epoch}\")\n",
        "            print(\"Available checkpoints:\")\n",
        "            for epoch in sorted(saved_checkpoints):\n",
        "                print(f\"Epoch {epoch}\")\n",
        "            resume_epoch = int(input(\"Enter the epoch number to resume training from: \"))\n",
        "            checkpoint.restore(f\"{CHECK_POINT_PATH}/ckpt-{resume_epoch}\")\n",
        "            print(f\"Restored checkpoint from epoch {resume_epoch}\")\n",
        "        else:\n",
        "            print(\"[*] training from scratch.\")\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(lr, hr):\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_hr = model(lr, training=True)\n",
        "            loss = W_PIXEL * pixel_loss(hr, generated_hr)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    wandb_run_id = \"psnr-training\" #@param {type:\"string\"}\n",
        "    if HAS_WANDB_ACCOUNT:\n",
        "        wandb.init(entity='ilab', project=PROJECT, id=wandb_run_id)\n",
        "    else:\n",
        "        wandb.init(id=wandb_run_id)\n",
        "\n",
        "    remain_steps = max(NUM_ITER - checkpoint.step.numpy(), 0)\n",
        "    pbar = tqdm(total=remain_steps, ncols=50)\n",
        "\n",
        "\n",
        "    for lr, hr in dataset.take(remain_steps):\n",
        "        checkpoint.step.assign_add(1)\n",
        "        steps = checkpoint.step.numpy()\n",
        "        loss = train_step(lr, hr)\n",
        "        learning_rate = optimizer.lr.numpy()  # Get the learning rate value\n",
        "        wandb.log({\"steps\": steps, \"loss\": loss, \"learning_rate\": learning_rate})\n",
        "        pbar.set_description(\"loss={:.4f}, lr={:.1e}\".format(loss, learning_rate))\n",
        "        pbar.update()\n",
        "        # sys.stdout.flush()\n",
        "        if steps % SAVE_STEPS == 0:\n",
        "           manager.save(checkpoint_number=steps)\n",
        "            print(f\"\\n[*] save ckpt file at {manager.latest_checkpoint}\")\n",
        "            model.save(f\"{SAVE_MODEL_PATH}/model-{steps}.h5\")\n",
        "            print(f\"\\n[*] save model at {SAVE_MODEL_PATH}/model-{steps}.h5\")\n",
        "\n",
        "\n",
        "\n",
        "          download_saved_folder(saved_folder_path=\"saved\", download_location=\"local\")\n",
        "          # files.download(\"saved.zip\")\n",
        "\n",
        "    model.save(SAVE_MODEL_PATH)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGW4Vmv_Jz69"
      },
      "outputs": [],
      "source": [
        "# train_esrgan.py\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "# from modules.esrgan import rrdb_net, discriminator_net\n",
        "# from modules.lr_scheduler import MultiStepLR\n",
        "# from modules.data import load_dataset\n",
        "# from modules.losses import get_pixel_loss, get_content_loss\n",
        "# from modules.losses import get_discriminator_loss, get_generator_loss\n",
        "\n",
        "HAS_WANDB_ACCOUNT = False\n",
        "PROJECT = 'esrgan-tf2'\n",
        "import wandb\n",
        "if not HAS_WANDB_ACCOUNT:\n",
        "    wandb.login(anonymous='allow')\n",
        "else:\n",
        "    wandb.login()\n",
        "\n",
        "INITIAL_LR_G = 1e-4\n",
        "INITIAL_LR_D = 1e-4\n",
        "LR_RATE = 0.5\n",
        "LR_STEPS = [5000, 10000, 20000, 30000]\n",
        "ADAM_BETA1_G = 0.9\n",
        "ADAM_BETA2_G = 0.99\n",
        "ADAM_BETA1_D = 0.9\n",
        "ADAM_BETA2_D = 0.99\n",
        "\n",
        "PIXEL_CRITERION = 'l1'\n",
        "FEATURE_CRITERION = 'l2'\n",
        "GAN_TYPE = 'ragan'\n",
        "WEIGHT_PIXEL = 1e-2\n",
        "WEIGHT_FEATURE = 1.0\n",
        "WEIGHT_GAN = 5e-3\n",
        "\n",
        "HR_HEIGHT = 128\n",
        "HR_WIDTH = 128\n",
        "SCALE = 4\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 10240\n",
        "INPUT_SHAPE=(None, None, 3)\n",
        "\n",
        "NUM_ITER = 4000\n",
        "SAVE_STEPS =  500\n",
        "\n",
        "PRETRAIN_PATH =  \"./saved/checkpoints/psnr\"\n",
        "CHECK_POINT_PATH =  \"./saved/checkpoints/esrgan\"\n",
        "Path(CHECK_POINT_PATH).mkdir(parents=True, exist_ok=True)\n",
        "SAVE_GAN_PATH = \"./saved/models/esrgan.h5\"\n",
        "Path(SAVE_GAN_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "SAVE_DISC_PATH = \"./saved/models/disc_gan.h5\"\n",
        "Path(SAVE_DISC_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    num_epochs = 0\n",
        "\n",
        "    dataset = load_dataset(HR_HEIGHT, HR_WIDTH, SCALE)\n",
        "    leng = len(dataset)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    generator = rrdb_net(input_shape=INPUT_SHAPE,scale_factor=SCALE)\n",
        "    discriminator = discriminator_net(input_shape=INPUT_SHAPE)\n",
        "\n",
        "    learning_rate_G = MultiStepLR(INITIAL_LR_G, LR_STEPS, LR_RATE)\n",
        "    learning_rate_D = MultiStepLR(INITIAL_LR_D, LR_STEPS, LR_RATE)\n",
        "    optimizer_G = tf.keras.optimizers.Adam(learning_rate= learning_rate_G,\n",
        "                                        beta_1= ADAM_BETA1_G,\n",
        "                                        beta_2= ADAM_BETA2_G\n",
        "                                        )\n",
        "    optimizer_D = tf.keras.optimizers.Adam(learning_rate= learning_rate_D,\n",
        "                                        beta_1= ADAM_BETA1_D,\n",
        "                                        beta_2= ADAM_BETA2_D\n",
        "                                        )\n",
        "\n",
        "    pixel_loss = get_pixel_loss(PIXEL_CRITERION)\n",
        "    feature_loss = get_content_loss(FEATURE_CRITERION)\n",
        "    generator_loss = get_generator_loss(GAN_TYPE)\n",
        "    discriminator_loss = get_discriminator_loss(GAN_TYPE)\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(step=tf.Variable(0, name='step'),\n",
        "                                     optimizer_G=optimizer_G,\n",
        "                                     optimizer_D=optimizer_D,\n",
        "                                     model=generator,\n",
        "                                     discriminator=discriminator)\n",
        "    manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
        "                                         directory=CHECK_POINT_PATH,\n",
        "                                         max_to_keep=3)\n",
        "    # if manager.latest_checkpoint:\n",
        "    #     checkpoint.restore(manager.latest_checkpoint)\n",
        "    #     print('[*] load ckpt from {} at step {}.'.format(\n",
        "    #         manager.latest_checkpoint, checkpoint.step.numpy()))\n",
        "    # else:\n",
        "    #     if tf.train.latest_checkpoint(PRETRAIN_PATH):\n",
        "    #         checkpoint.restore(tf.train.latest_checkpoint(PRETRAIN_PATH))\n",
        "    #         checkpoint.step.assign(0)\n",
        "    #         print(\"[*] training from pretrain model {}.\".format(\n",
        "    #                 PRETRAIN_PATH ))\n",
        "    #     else:\n",
        "    #         print(\"[*] cannot find pretrain model {}.\".format(\n",
        "    #             PRETRAIN_PATH))\n",
        "\n",
        "    saved_checkpoints = [int(ckpt.split('-')[-1]) for ckpt in os.listdir(CHECK_POINT_PATH)]\n",
        "    if saved_checkpoints:\n",
        "        latest_epoch = max(saved_checkpoints)\n",
        "        print(f\"Found saved checkpoints. Latest epoch: {latest_epoch}\")\n",
        "        print(\"Available checkpoints:\")\n",
        "        for epoch in sorted(saved_checkpoints):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "        resume_epoch = int(input(\"Enter the epoch number to resume training from: \"))\n",
        "        checkpoint.restore(f\"{CHECK_POINT_PATH}/ckpt-{resume_epoch}\")\n",
        "        print(f\"Restored checkpoint from epoch {resume_epoch}\")\n",
        "    else:\n",
        "        if tf.train.latest_checkpoint(PRETRAIN_PATH):\n",
        "            checkpoint.restore(tf.train.latest_checkpoint(PRETRAIN_PATH))\n",
        "            checkpoint.step.assign(0)\n",
        "            print(\"[*] training from pretrain model {}.\".format(\n",
        "                    PRETRAIN_PATH ))\n",
        "        else:\n",
        "            print(\"[*] cannot find pretrain model {}.\".format(\n",
        "                PRETRAIN_PATH))\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(lr, hr):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            generated_hr = generator(lr, training=True)\n",
        "            real_logits = discriminator(hr, training=True)\n",
        "            fake_logits = discriminator(generated_hr, training=True)\n",
        "            losses_G = {}\n",
        "            losses_D = {}\n",
        "            losses_G['pixel'] = WEIGHT_PIXEL * pixel_loss(hr, generated_hr)\n",
        "            losses_G['feature'] = WEIGHT_FEATURE * feature_loss(hr, generated_hr)\n",
        "            losses_G['gan'] = WEIGHT_GAN * generator_loss(real_logits, fake_logits)\n",
        "            losses_D['disc'] = discriminator_loss(real_logits, fake_logits)\n",
        "            total_loss_G = tf.add_n([l for l in losses_G.values()])\n",
        "            total_loss_D = tf.add_n([l for l in losses_D.values()])\n",
        "\n",
        "\n",
        "        grads_G = tape.gradient(\n",
        "            total_loss_G, generator.trainable_variables)\n",
        "        grads_D = tape.gradient(\n",
        "            total_loss_D, discriminator.trainable_variables)\n",
        "        optimizer_G.apply_gradients(\n",
        "            zip(grads_G, generator.trainable_variables))\n",
        "        optimizer_D.apply_gradients(\n",
        "            zip(grads_D, discriminator.trainable_variables))\n",
        "\n",
        "        return total_loss_G, total_loss_D, losses_G, losses_D\n",
        "\n",
        "\n",
        "    wandb_run_id = \"esrgan-training\" #@param {type:\"string\"}\n",
        "    if HAS_WANDB_ACCOUNT:\n",
        "        wandb.init(entity='ilab', project=PROJECT, id=wandb_run_id)\n",
        "    else:\n",
        "        wandb.init(id=wandb_run_id)\n",
        "    remain_steps = max(NUM_ITER - checkpoint.step.numpy(), 0)\n",
        "    pbar = tqdm(total=remain_steps, ncols=50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for lr, hr in dataset.take(remain_steps):\n",
        "        checkpoint.step.assign_add(1)\n",
        "        steps = checkpoint.step.numpy()\n",
        "        total_loss_G, total_loss_D, losses_G, losses_D = train_step(lr, hr)\n",
        "        learning_rate_G = optimizer_G.lr.numpy()\n",
        "        learning_rate_D = optimizer_D.lr.numpy()\n",
        "        wandb.log({**{\"steps\": steps},**losses_G, **losses_D,\n",
        "                    **{\"total_loss_G\": total_loss_G.numpy()},\n",
        "                    **{\"learning_rate_G\": learning_rate_G,\n",
        "                    \"learning_rate_D\": learning_rate_D}})\n",
        "\n",
        "        pbar.set_description(\"loss_G={:.4f}, loss_D={:.4f}, lr_G={:.1e}, lr_D={:.1e}\".format(\n",
        "            total_loss_G.numpy(), total_loss_D.numpy(),\n",
        "            learning_rate_G, learning_rate_D))\n",
        "        pbar.update()\n",
        "        if steps % SAVE_STEPS == 0:\n",
        "            manager.save(checkpoint_number=steps)\n",
        "            print(f\"\\n[*] save ckpt file at {manager.latest_checkpoint}\")\n",
        "            generator.save(f\"{SAVE_GAN_PATH}/generator-{steps}.h5\")\n",
        "            discriminator.save(f\"{SAVE_DISC_PATH}/discriminator-{steps}.h5\")\n",
        "            print(f\"[*] save generator at {SAVE_GAN_PATH}/generator-{steps}.h5\")\n",
        "            print(f\"[*] save discriminator at {SAVE_DISC_PATH}/discriminator-{steps}.h5\")\n",
        "\n",
        "            download_saved_folder(saved_folder_path=\"saved\", download_location=\"local\")\n",
        "\n",
        "    generator.save(SAVE_GAN_PATH)\n",
        "    discriminator.save(SAVE_DISC_PATH)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP9SuiyEzmle"
      },
      "outputs": [],
      "source": [
        "# net_interp.py\n",
        "\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "# from modules.esrgan import rrdb_net\n",
        "\n",
        "\n",
        "SCALE = 4\n",
        "INPUT_SHAPE=(None, None, 3)\n",
        "ALPHA = 0.8\n",
        "\n",
        "CHECKPOINT_PATH_PSNR = \"./saved/checkpoints/psnr\"\n",
        "CHECKPOINT_PATH_ESRGAN = \"./saved/checkpoints/esrgan\"\n",
        "SAVE_MODEL_PATH = \"./saved/models/interp_esr.h5\"\n",
        "Path(SAVE_MODEL_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "\n",
        "    # define network\n",
        "    model = rrdb_net(input_shape=INPUT_SHAPE,scale_factor=SCALE)\n",
        "\n",
        "    # load checkpoint\n",
        "    checkpoint_psnr = tf.train.Checkpoint(model=model)\n",
        "    if tf.train.latest_checkpoint(CHECKPOINT_PATH_PSNR):\n",
        "        status = checkpoint_psnr.restore(tf.train.latest_checkpoint(CHECKPOINT_PATH_PSNR))\n",
        "        status.expect_partial()\n",
        "        print(\"[*] load ckpt psnr from {}.\".format(\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_PATH_PSNR)))\n",
        "    else:\n",
        "        print(\"[*] Cannot find ckpt psnr from {}.\".format(\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_PATH_PSNR)))\n",
        "        exit()\n",
        "    vars_psnr = [v.numpy() for v in checkpoint_psnr.model.trainable_variables]\n",
        "\n",
        "    checkpoint_esrgan = tf.train.Checkpoint(model=model)\n",
        "    if tf.train.latest_checkpoint(CHECKPOINT_PATH_ESRGAN):\n",
        "        status = checkpoint_esrgan.restore(tf.train.latest_checkpoint(CHECKPOINT_PATH_ESRGAN))\n",
        "        status.expect_partial()\n",
        "        print(\"[*] load ckpt edsr from {}.\".format(\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_PATH_ESRGAN)))\n",
        "    else:\n",
        "        print(\"[*] Cannot find ckpt edsr from {}.\".format(\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_PATH_ESRGAN)))\n",
        "        exit()\n",
        "    vars_edsr = [v.numpy() for v in checkpoint_esrgan.model.trainable_variables]\n",
        "\n",
        "    # network interpolation\n",
        "    for i, var in enumerate(model.trainable_variables):\n",
        "        var.assign((1 - ALPHA) * vars_psnr[i] + ALPHA * vars_edsr[i])\n",
        "\n",
        "    model.save(SAVE_MODEL_PATH)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbbvgasNgDT-"
      },
      "outputs": [],
      "source": [
        "#demo.py (esrgan evaluate)\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "# from modules.esrgan import rrdb_net\n",
        "# from modules.utils import read_image, scale_image_0_1_range, tensor2img\n",
        "# from modules.utils import save_image_grid\n",
        "\n",
        "\n",
        "\n",
        "SCALE = 4\n",
        "INPUT_SHAPE=(None, None, 3)\n",
        "\n",
        "FROM_CHECKPOINT = True\n",
        "#MODEL_PATH = \"./saved/models/psnr.h5\"\n",
        "MODEL_PATH = \"./saved/models/esrgan.h5\"\n",
        "#CHECKPOINT_PATH = \"./saved/checkpoints/psnr\"\n",
        "CHECKPOINT_PATH = \"./saved/checkpoints/esrgan\"\n",
        "\n",
        "IMG_DIR = \"./images/input\"  # Path to the directory containing LR images\n",
        "GT_DIR = \"./images/ground_truth\"  # Path to the directory containing ground truth HR images (optional)\n",
        "SAVE_DIR = \"./images/results\"  # Directory to save the upscaled images\n",
        "\n",
        "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "\n",
        "    model = rrdb_net(input_shape=INPUT_SHAPE,scale_factor=SCALE)\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(model=model)\n",
        "\n",
        "    if tf.train.latest_checkpoint(CHECKPOINT_PATH) and FROM_CHECKPOINT:\n",
        "            status = checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_PATH))\n",
        "            status.expect_partial()  # Add this line to suppress warnings\n",
        "            print(\"[*] load ckpt from {}.\".format(\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_PATH)))\n",
        "    else:\n",
        "        if os.path.isfile(MODEL_PATH):\n",
        "            h5_model = load_model(MODEL_PATH, custom_objects={'tf': tf})\n",
        "            weights = h5_model.get_weights()\n",
        "            model.set_weights(weights)\n",
        "            print(\"[*] load model weights from {}.\".format(\n",
        "            MODEL_PATH))\n",
        "        else:\n",
        "            print(\"[*] Cannot find ckpt or h5 model file.\")\n",
        "            exit()\n",
        "\n",
        "    if os.path.isdir(IMG_DIR):  # Check if it's a directory\n",
        "        for img_path in glob.glob(os.path.join(IMG_DIR, \"*.png\")):  # Assuming PNG images\n",
        "            # Extract filename without extension\n",
        "            filename = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "             # Load LR image\n",
        "            lr_image = read_image(img_path)\n",
        "            lr_image = scale_image_0_1_range(lr_image)\n",
        "            lr_image = tf.expand_dims(lr_image, axis=0)\n",
        "\n",
        "            base_filename = filename[2:]\n",
        "\n",
        "            # Load ground truth HR image (if available)\n",
        "            hr_filename = f\"hr{base_filename}.png\"  # Add \"hr\" prefix and extension\n",
        "            hr_img_path = os.path.join(GT_DIR, hr_filename)\n",
        "            if os.path.exists(hr_img_path):\n",
        "                hr_image = read_image(hr_img_path)\n",
        "            else:\n",
        "                hr_image = None\n",
        "\n",
        "            # Generate HR image\n",
        "            generated_hr = model(lr_image)\n",
        "            generated_hr_image = tensor2img(generated_hr)\n",
        "            unscale_lr_image = tensor2img(lr_image)\n",
        "\n",
        "            # Calculate metrics (if ground truth is available)\n",
        "            if hr_image is not None:\n",
        "                psnr = calculate_psnr(hr_image, generated_hr_image)\n",
        "                ssim = calculate_ssim(hr_image, generated_hr_image)\n",
        "                print(f\"[***] Image: {filename}, PSNR: {psnr}, SSIM: {ssim}\")\n",
        "\n",
        "            # Save image grid with LR, generated HR, and optionally ground truth\n",
        "            save_path = os.path.join(SAVE_DIR, f\"{filename}_upscaled.png\")\n",
        "            save_image_grid(unscale_lr_image, generated_hr_image, hr_image, save_path=save_path)\n",
        "\n",
        "    else:\n",
        "        print(f\"[!] Invalid image directory: {IMG_DIR}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IWMT3VRzwNn"
      },
      "outputs": [],
      "source": [
        "#test.py (interp_esr)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "# from modules.esrgan import rrdb_net\n",
        "# from modules.utils import create_lr_hr_pair, scale_image_0_1_range, tensor2img\n",
        "# from modules.utils import save_image_grid\n",
        "# from modules.metrics import calculate_psnr, calculate_ssim\n",
        "\n",
        "\n",
        "SCALE = 4\n",
        "INPUT_SHAPE=(None, None, 3)\n",
        "\n",
        "FROM_CHECKPOINT = False\n",
        "#MODEL_PATH = \"./saved/models/psnr.h5\"\n",
        "# MODEL_PATH = \"./saved/models/esrgan.h5\"\n",
        "MODEL_PATH = \"./saved/models/interp_esr.h5\"\n",
        "#CHECKPOINT_PATH = \"./saved/checkpoints/psnr\"\n",
        "CHECKPOINT_PATH = \"./saved/checkpoints/esrgan\"\n",
        "\n",
        "IMG_DIR = \"./images/input\"  # Path to the directory containing LR images\n",
        "GT_DIR = \"./images/ground_truth\"  # Path to the directory containing ground truth HR images\n",
        "SAVE_DIR = \"./images/results\"  # Directory to save the upscaled images\n",
        "\n",
        "Path(SAVE_DIR).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "\n",
        "    model = rrdb_net(input_shape=INPUT_SHAPE,scale_factor=SCALE)\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(model=model)\n",
        "    if tf.train.latest_checkpoint(CHECKPOINT_PATH) and FROM_CHECKPOINT:\n",
        "        checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_PATH))\n",
        "        print(\"[*] load ckpt from {}.\".format(\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_PATH)))\n",
        "    else:\n",
        "        if os.path.isfile(MODEL_PATH):\n",
        "            h5_model = load_model(MODEL_PATH, custom_objects={'tf': tf})\n",
        "            weights = h5_model.get_weights()\n",
        "            model.set_weights(weights)\n",
        "            print(\"[*] load model weights from {}.\".format(\n",
        "            MODEL_PATH))\n",
        "        else:\n",
        "            print(\"[*] Cannot find ckpt or h5 model file.\")\n",
        "            exit()\n",
        "\n",
        "    if os.path.isdir(IMG_DIR):  # Check if it's a directory\n",
        "        for img_path in glob.glob(os.path.join(IMG_DIR, \"*.png\")):  # Assuming PNG images\n",
        "            # Extract filename without extension\n",
        "            filename = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "            # Load LR image\n",
        "            lr_image = read_image(img_path)\n",
        "            lr_image = scale_image_0_1_range(lr_image)\n",
        "            lr_image = tf.expand_dims(lr_image, axis=0)\n",
        "\n",
        "            base_filename = filename[2:]\n",
        "\n",
        "            # Load ground truth HR image\n",
        "            hr_filename = f\"hr{base_filename}.png\"  # Add \"hr\" prefix and extension\n",
        "            hr_img_path = os.path.join(GT_DIR, hr_filename)\n",
        "            hr_image = read_image(hr_img_path)\n",
        "\n",
        "\n",
        "            # Generate HR image\n",
        "            generated_hr = model(lr_image)\n",
        "            generated_hr_image = tensor2img(generated_hr)\n",
        "            unscale_lr_image = tensor2img(lr_image)\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr = calculate_psnr(hr_image, generated_hr_image)\n",
        "            ssim = calculate_ssim(hr_image, generated_hr_image)\n",
        "            print(f\"[***] Image: {filename}, PSNR: {psnr}, SSIM: {ssim}\")\n",
        "\n",
        "            # Save image grid\n",
        "            save_path = os.path.join(SAVE_DIR, f\"{filename}_upscaled.png\")\n",
        "            save_image_grid(unscale_lr_image, generated_hr_image,  hr_image)\n",
        "\n",
        "    else:\n",
        "        print(f\"[!] Invalid image directories: {IMG_DIR}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxe8GnZAXOix"
      },
      "outputs": [],
      "source": [
        "# !pip install zipfile\n",
        "# import os\n",
        "# import zipfile\n",
        "# from google.colab import files\n",
        "\n",
        "# def zip_folder(folder_path, output_path):\n",
        "#     \"\"\"Zip the contents of an entire folder (with that folder included\n",
        "#     in the archive). Empty subfolders will be included in the archive\n",
        "#     as well.\n",
        "#     \"\"\"\n",
        "#     parent_folder = os.path.dirname(folder_path)\n",
        "#     # Retrieve the paths of the folder contents.\n",
        "#     contents = os.walk(folder_path)\n",
        "#     try:\n",
        "#         zip_file = zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED)\n",
        "#         for root, folders, files in contents:\n",
        "#             # Include all subfolders, including empty ones.\n",
        "#             for folder_name in folders:\n",
        "#                 absolute_path = os.path.join(root, folder_name)\n",
        "#                 relative_path = absolute_path.replace(parent_folder + '/', '')\n",
        "#                 print(\"Adding {} to archive.\".format(absolute_path))\n",
        "#                 zip_file.write(absolute_path, relative_path)\n",
        "#             for file_name in files:\n",
        "#                 absolute_path = os.path.join(root, file_name)\n",
        "#                 relative_path = absolute_path.replace(parent_folder + '/', '')\n",
        "#                 print(\"Adding {} to archive.\".format(absolute_path))\n",
        "#                 zip_file.write(absolute_path, relative_path)\n",
        "#         print(\"'{0}' created successfully.\".format(output_path))\n",
        "#     except IOError as message:\n",
        "#         print(message)\n",
        "#         sys.exit(1)\n",
        "#     except OSError as message:\n",
        "#         print(message)\n",
        "#         sys.exit(1)\n",
        "#     finally:\n",
        "#         zip_file.close()\n",
        "\n",
        "# zip_folder(\"saved\", \"saved.zip\")\n",
        "\n",
        "# files.download(\"saved.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIwmGBMy8dIt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2_jGTegXYy7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "427b24f5b0444c75a0dd47069583d305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42a16da46407469090f26457cf77623c",
              "IPY_MODEL_2d7c8b967a964aa7af221bed3020a98f"
            ],
            "layout": "IPY_MODEL_76c76805c7c447b795282b450ffa002a"
          }
        },
        "42a16da46407469090f26457cf77623c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f14f780a64f4c7e89fbfde169ab442b",
            "placeholder": "​",
            "style": "IPY_MODEL_f2b2f67a63524f0691777d2ba4f8ea5f",
            "value": "0.010 MB of 0.010 MB uploaded\r"
          }
        },
        "2d7c8b967a964aa7af221bed3020a98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffe79bf4d140477fb3a87b8cc6189181",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77ed74671ce2427a90c9c47cbf129509",
            "value": 1
          }
        },
        "76c76805c7c447b795282b450ffa002a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f14f780a64f4c7e89fbfde169ab442b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2b2f67a63524f0691777d2ba4f8ea5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffe79bf4d140477fb3a87b8cc6189181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77ed74671ce2427a90c9c47cbf129509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}